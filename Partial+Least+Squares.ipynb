{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\begin{center}\n",
    "\\author{Jonathan Bryan, Carmen Dekmezian}\n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Partial least squares (PLS) is a practical algorithm for regression when modeling high-dimensional data that includes high collinearity. PLS was developed in the mid-20th century by the econometrician Herman Wold and subsequently widely adopted as a tool for the quantitative analysis of compounds in chemistry. PLS has the capacity for high predictive accuracy as it decomposes the matrix of predictor and response values into two separate blocks. These separate blocks represent latent factors or components that most account for the variation within the data. This process is guided by an inner relation between predictor scores, response scores, and regression coefficient that assures each decomposition of components maximizes its relationship to the response variable and includes information from the previous component. The model then normally uses a lower dimensional subspace of the design matrix to predict future response values, a projection of the latent structure. This gives the PLS high predictive capacity but lower ability for interpretation of components and therefore important variables. In this report, PLS is used to model housing data from Ames, Iowa which incorporates several predictors of different housing attributes to predict home price. Comparison of PLS to principal component regression (PCR), and ordinary least squares (OLS) is done. Simulated data that includes more predictors than observations and high collinearity are produced and latent components are recovered using PLS to allow for prediction. Finally, optimization of the algorithm is demonstrated using just-in-time programming and efficient functional and memory technique.\n",
    "\n",
    "Our code can be found in the github repository: https://github.com/cdek11/PLS\n",
    "\n",
    "# Background\n",
    "The paper we selected is titled “Partial Least Squares Regression: A Tutorial” by Geladi and Kowalski.$^1$ It discusses partial least squares regression (PLSR), which is a method for decomposing values of predictors and response into more basic components and finding a robust linear relationship between the two components. PLSR was formulated by econometrician Herman Wold as a technique for \"soft modelling\", when the number of variables is high, the relationship between the variables is poorly understood and prediction is the primary goal.$^2$ His son Svante Wold extended the use of PLSR to chemometrics to model high dimensional chemical data.\n",
    "\n",
    "The nonexistence of a unique solution when the number of predictors is larger than the sample size of the data is a challenging problem in multiple linear regression, especially when multicollinearity and missing are also issues. Other methods that seek to reduce the dimensionality of the predictors, such as principal component analysis (PCR), allow for noise reduction and solve the collinearity problem. However, useful information for accurate prediction can be lost because PCR explains the useful directional information in the predictor space which may not be sufficiently linked to the space of new observed responses.\n",
    "\n",
    "PLSR produces X-scores, much like PCR, but also Y-scores that explain the given response space. These scores are generated by seeking important directions in the X-scores that are strongly associated with variation in the Y-scores and biased towards accurate predictions.\n",
    "\n",
    "PLSR has several advantages over other methods. It is advantageous when the number of predictors is larger than the sample size and collinearity is high among the predictors. It is also a robust method regression because it reduces out of sample variance of residual errors and noise in the data in comparison to common multiple least squares regression algorithms. Additionally, it is beneficial when there are missing observations in the data. \n",
    "\n",
    "One disadvantage of PLSR is that the use of lower dimensional representations of the data and loadings of the model can make the interpretability of the important latent predictors difficult in some situations.\n",
    "\n",
    "## Description of algorithm\n",
    "\n",
    "PLS is a regression method used to overcome limitations discussed above for normal linear regressions (e.g., many collinear predictors, more predictors than samples, etc.) by mapping observed sets of observed variables to response variables by means of latent variables.$^3$ Essentially the model assumes that the data is generated by an underlying model directed by a smaller number of latent variables in the data. \n",
    "\n",
    "First, two sets of latent variables are extracted from the data: $T$ (or x-scores) from the predictors, and $U$ (or y-scores) from the response variable. These latent vectors are determined through maximizing the covariance between different sets of variables. \n",
    "\n",
    "For the classic linear regression, we try to solve the equation, $Y = X\\beta + \\epsilon$, where the ordinary least squares estimate for $\\beta$ is identified as $(X^T X)^{-1} X^TY$.$^4$ This estimate is obtained by minimizing the sum of squared residuals. However, models that have predictors with high collinearity or more predictors than observations can result in singularity of the matrix $(X^T X)$. As an alternative and way to fix this issue, we implement the PLS algorithm throught the following steps$^2$:\n",
    "\n",
    "1) Start with vector $u$. If there is only one response variable, then $u = y$, otherwise it is one of the columns of $Y$.\n",
    "\n",
    "2) Calculate the weights for the predictors ($X$).\n",
    "$$w = \\frac{X^Tu}{u^Tu}$$\n",
    "\n",
    "3) Normalize the weights for the predictors ($X$).\n",
    "$$w = \\frac{w}{||w||}$$\n",
    "\n",
    "4) Determine $t$ ($X$ scores).\n",
    "\n",
    "$$t = \\frac{Xw}{t^{T}t} $$\n",
    "\n",
    "5) Now perform similar calculations for $Y$. Calculate the weights for the response variable.\n",
    "$$c = \\frac{X^Tt}{t^{T}t}$$\n",
    "\n",
    "6) Normalize the weights for the response ($X$):\n",
    "$$c = \\frac{c}{||c||}$$\n",
    "\n",
    "7) Determine $u$ ($Y$ scores).\n",
    "$$u = \\frac{Yc}{c^Tc}$$\n",
    "\n",
    "8) If there is more than one response variable, then we test to determine whether the $t$ values have converged. If the change in $t$ from one iteration to the next, $\\frac{||t_{old} - t_{new}||}{||t_{new}||}$, is not smaller than a threshold value, then we iterate through steps 2-5 until convergence is reached.\n",
    "\n",
    "9) Deflate variables for next iteration.\n",
    "$$p = \\frac{X^Tt}{t^Tt}$$\n",
    "$$X = X - tp^T$$\n",
    "$$Y = Y - tc^T$$\n",
    "\n",
    "10) Calculate the component regression coefficient.\n",
    "$$ b = \\frac{u^{T}t}{t^{T}t}$$\n",
    "\n",
    "11) Calculate the X and Y residuals for component h\n",
    "$$E_h = E_{h-1} - t_h p_h; X = E_{0}$$\n",
    "$$F_h = F_{h-1} - b_h t_h q_{h}^T; Y = F_{0} $$\n",
    "\n",
    "12) Iterate through components until they are not found to be predictive of $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "%matplotlib inline\n",
    "import time\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression as lin_reg\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in ames home price train and test data\n",
    "ames_train = pd.read_csv(\"ames_train.csv\")\n",
    "ames_test = pd.read_csv(\"ames_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge both files first so there are the same number of categories created\n",
    "ames_merged = pd.concat([ames_train, ames_test])\n",
    "ames_merged = pd.get_dummies(ames_merged).fillna(value=0)\n",
    "\n",
    "# save merged file to csv in folder\n",
    "ames_merged[0:1500].to_csv(\"ames_data_train.csv\")\n",
    "ames_merged[1500:2000].to_csv(\"ames_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert categorical variables to dummy variables\n",
    "# create variables for train and test data separately, x for predictors, y for response\n",
    "x_predictors_train = ames_merged[0:1500].drop([\"price\", \"PID\"], axis = 1)\n",
    "x_predictors_test = ames_merged[1500:2000].drop([\"price\", \"PID\"], axis = 1)\n",
    "\n",
    "y_train = ames_train.price\n",
    "y_test = ames_test.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm: Partial Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pls(path, path_test, predictors, response):\n",
    "    '''Function that takes a dataframe and runs partial least squares on numeric predictors for a numeric response.\n",
    "    Returns the residuals of the predictor (X block), response (Y block), and traininig RMSE'''\n",
    "    combined = predictors #Ready list to combine the predictors and response to get both sets of data\n",
    "    ###Data preparation\n",
    "    data = pd.DataFrame.from_csv(path) #Retrieve full csv data from local machine\n",
    "    combined.append(response) #Add the response to the list of variables to get from data set\n",
    "    data = data[combined] #Only retrieve the predictors and response\n",
    "    response_std = data[response].std() #Store the response variable standard deviation to scale the RMSE to real units at end\n",
    "    \n",
    "    #Subtract the mean from each column\n",
    "    data = data - data.mean()\n",
    "\n",
    "    #Scale each column by the column standard deviation\n",
    "    data = data/data.std()\n",
    "\n",
    "    #Separate in to design matrix (X block) and response column vector (Y block)\n",
    "    predictors.pop() #Remove the response variable from the predictors list\n",
    "    X = data[predictors].as_matrix() #Create a matrix of predictor values\n",
    "    Y = data[[response]].as_matrix() #Create a matrix of predictor values\n",
    "    Y_true = Y #Store the true Y values for prediction later\n",
    "    \n",
    "    #Get rank of matrix\n",
    "    rank = np.linalg.matrix_rank(X) #Store rank of matrix because this is the maximum number of components the model can have\n",
    "    \n",
    "    #PLS algorithm\n",
    "    u = Y #Set intital u value as response variables\n",
    "    Xres_dictionary = {} #Create a dictionary for the residuals from the decomposition of the X block\n",
    "    Yres_dictionary = {} #Create a dictionary for the residuals from the decomposition of the Y block\n",
    "    q_new_dictionary ={} #Create a dictionary for row vectors of q loadings for the Y block\n",
    "    b_dictionary = {} #Create a dictionary for scalar regression coefficient for PLS components \n",
    "    t_hat_dictionary = {} #Create a dictionary for the matrix of X scores \n",
    "    t_hat_train_dictionary = {} #Create a dictionary for the matrix of X scores for training data\n",
    "    t_hat_test_dictionary = {} #Create a dictionary for the matrix of X scores for test data\n",
    "    RMSE_dictionary = {} #Create a dictionary to store RMSE for training data\n",
    "    RMSE_test_dictionary = {} #Create a dictionary to store RMSE for test data\n",
    "    for i in range(1,(rank+1)):\n",
    "        Y_pred = np.zeros((Y_true.shape[0],1))\n",
    "        #Here we have one variable in the Y block so q = 1 \n",
    "        #and omit steps 5-8\n",
    "        q = 1\n",
    "\n",
    "        #For the X block, u = Y\n",
    "        u = Y #random y column from Y #Step 1\n",
    "        w_old = np.dot(u.T,X)/np.dot(u.T,u) #Step 2\n",
    "        w_new = w_old/np.linalg.norm(w_old) #Step 3\n",
    "        t = np.dot(X,w_new.T)/np.dot(w_new,w_new.T) #Step 4\n",
    "\n",
    "        #For the Y block can be omitted if Y only has one variable\n",
    "        q_old = np.dot(t.T,Y)/np.dot(t.T,t) #Step 5\n",
    "        q_new = q_old/np.linalg.norm(q_old) #Step 6\n",
    "        q_new_dictionary[i] = q_new\n",
    "        u = np.dot(Y,q_new.T)/np.dot(q_new,q_new.T) #Step 7\n",
    "\n",
    "        #Step 8: Check convergence\n",
    "\n",
    "        #Calculate the X loadings and rescale the scores and weights accordingly\n",
    "        p = np.dot(t.T,X)/np.dot(t.T,t) #Step 9\n",
    "        p_new = p.T/np.linalg.norm(p.T) #Step 10\n",
    "        t_new = t/np.linalg.norm(p.T) #Step 11\n",
    "        w_new = w_old/np.linalg.norm(p)  #Step 12\n",
    "\n",
    "        #Find the regression coefficient for b for th inner relation\n",
    "        b = np.dot(u.T,t_new)/np.dot(t.T,t) #Step 13\n",
    "        b_dictionary[i] = b\n",
    "\n",
    "        #Calculation of the residuals\n",
    "        E_h = X - np.dot(t_new,p_new.T)\n",
    "        F_h = Y - b.dot(t_new.T).T.dot(q)\n",
    "        \n",
    "        #Set outer relation for the X block\n",
    "        Xres_dictionary[i] = E_h\n",
    "        X = E_h\n",
    "        \n",
    "        #Set the mixed relation for the Y block\n",
    "        Yres_dictionary[i] = F_h\n",
    "        Y = F_h\n",
    "        \n",
    "        #Find estimated t hat\n",
    "        t_hat = np.dot(E_h,w_new.T)\n",
    "        t_hat_dictionary[i] = t_hat\n",
    "        E_h = E_h - np.dot(t_hat,p_new.T)\n",
    "        \n",
    "        #Predict training set response by summing over different compenents\n",
    "        E_h = X\n",
    "        for j in range(1,i+1):\n",
    "            t_hat_train = np.dot(E_h,w_new.T)\n",
    "            t_hat_train_dictionary[j] = t_hat_train\n",
    "            E_h = E_h - np.dot(t_hat_train, p_new.T)\n",
    "            for g in range(1,i+1):\n",
    "                Y_pred = Y_pred + (b_dictionary[g]*t_hat_dictionary[g]).dot(q_new_dictionary[g].T)\n",
    "        \n",
    "        #Find training RMSE \n",
    "        RMSE = np.sqrt(sum((Y_true - Y_pred)**2)/Y_true.shape[0]) \n",
    "        RMSE_scaled = RMSE * response_std \n",
    "        RMSE_dictionary[i] = RMSE_scaled\n",
    "        \n",
    "        #Code chunk to find test RMSE\n",
    "        #Load data\n",
    "        data_test = pd.DataFrame.from_csv(path_test)\n",
    "        combined.append(response)\n",
    "        data_test = data_test[combined]\n",
    "        response_std_test = data_test[response].std()\n",
    "    \n",
    "        #Subtract the mean from each column\n",
    "        data_test = data_test - data_test.mean()\n",
    "\n",
    "        #Scale each column by the column standard deviation\n",
    "        data_test = data_test/data_test.std()\n",
    "\n",
    "        #Separate in to design matrix (X block) and response column vector (Y block)\n",
    "        predictors.pop()\n",
    "        X_test = data[predictors].as_matrix()\n",
    "        Y_test = data[[response]].as_matrix()\n",
    "        Y_true_test = Y_test #For prediction\n",
    "        \n",
    "        Y_pred_test = np.zeros((Y_true_test.shape[0],1)) \n",
    "        \n",
    "        #Get rank of matrix\n",
    "        rank_test = np.linalg.matrix_rank(X)\n",
    "        \n",
    "        E_h_test = X_test\n",
    "        \n",
    "        #Sum over different compenents\n",
    "        for k in range(1,i+1):\n",
    "            t_hat_test = np.dot(E_h_test,w_new.T)\n",
    "            t_hat_test_dictionary[k] = t_hat_test\n",
    "            E_h_test = E_h_test - np.dot(t_hat_test, p_new.T)\n",
    "            Y_pred_test = Y_pred_test + (b_dictionary[k]*t_hat_test_dictionary[k]).dot(q_new_dictionary[k].T)\n",
    "        \n",
    "        #Find test RMSE \n",
    "        RMSE = np.sqrt(sum((Y_true_test - Y_pred_test)**2)/Y_true_test.shape[0]) \n",
    "        RMSE_scaled_test = RMSE * response_std_test # I believe this is the RMSE since the Y had to be scaled.\n",
    "        RMSE_test_dictionary[i] = RMSE_scaled_test\n",
    "        \n",
    "    return RMSE_dictionary, RMSE_test_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe optimization for performance\n",
    "\n",
    "In our initial version of the PLS algorithm, we tried to utilize numpy as much as possible, especially when dealing with matrices. In class, we learned about two common methods to compile the python code that we write, Just-In-Time (JIT) and Cython for Ahead-of-Time (AOT) compilation, but we ultimately decided to use the JIT approach. We first identified certain chunks of our main PLS function that could be easily separated and put them into their own functions. Then we used the @jit command before those specific functions, which allowed numba to effectively figure out when and how the code should be optimized. We ran our test data on both the initial PLS algorithm and then on the optimized version of the PLS algorithm for speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def mean_center_scale(dataframe):\n",
    "    '''Scale dataframe by subtracting mean and dividing by standard deviation'''\n",
    "    dataframe = dataframe - dataframe.mean()\n",
    "    dataframe = dataframe/dataframe.std()\n",
    "    return dataframe\n",
    "\n",
    "@jit\n",
    "def y_pred(Y_pred, i,b_dictionary,t_hat_dictionary,q_new_dictionary):\n",
    "    '''Find prediction for Y based on the number of components in this iteration'''\n",
    "    for j in range(1,i+1):\n",
    "        Y_pred = Y_pred + (b_dictionary[j]*t_hat_dictionary[j]).dot(q_new_dictionary[j].T)\n",
    "    return Y_pred    \n",
    "\n",
    "@jit\n",
    "def rmse(i,Y_true, Y_pred, response_std, RMSE_dictionary):\n",
    "    '''Find training RMSE''' \n",
    "    RMSE = np.sqrt(sum((Y_true - Y_pred)**2)/Y_true.shape[0])\n",
    "    RMSE_scaled = RMSE * response_std \n",
    "    RMSE_dictionary[i] = RMSE_scaled\n",
    "\n",
    "    return RMSE_dictionary\n",
    "\n",
    "@jit        \n",
    "def core_pls(i,Y, X, q_new_dictionary, b_dictionary, t_hat_dictionary) :\n",
    "    '''Core PLS algorithm'''\n",
    "    \n",
    "    #Here we have one variable in the Y block so q = 1 \n",
    "    #and omit steps 5-8\n",
    "    q = 1\n",
    "\n",
    "    #For the X block, u = Y\n",
    "    u = Y #random y column from Y #Step 1\n",
    "    w_old = np.dot(u.T,X)/np.dot(u.T,u) #Step 2\n",
    "    w_new = w_old/np.linalg.norm(w_old) #Step 3\n",
    "    t = np.dot(X,w_new.T)/np.dot(w_new,w_new.T) #Step 4\n",
    "\n",
    "    #For the Y block can be omitted if Y only has one variable\n",
    "    q_old = np.dot(t.T,Y)/np.dot(t.T,t) #Step 5\n",
    "    q_new = q_old/np.linalg.norm(q_old) #Step 6\n",
    "    q_new_dictionary[i] = q_new\n",
    "    u = np.dot(Y,q_new.T)/np.dot(q_new,q_new.T) #Step 7\n",
    "\n",
    "    #Step 8: Check convergence\n",
    "\n",
    "    #Calculate the X loadings and rescale the scores and weights accordingly\n",
    "    p = np.dot(t.T,X)/np.dot(t.T,t) #Step 9\n",
    "    p_new = p.T/np.linalg.norm(p.T) #Step 10\n",
    "    t_new = t/np.linalg.norm(p.T) #Step 11\n",
    "    w_new = w_old/np.linalg.norm(p)  #Step 12\n",
    "\n",
    "    #Find the regression coefficient for b for th inner relation\n",
    "    b = np.dot(u.T,t_new)/np.dot(t.T,t) #Step 13\n",
    "    b_dictionary[i] = b\n",
    "\n",
    "    #Calculation of the residuals\n",
    "    E_h = X - np.dot(t_new,p_new.T)\n",
    "    F_h = Y - b.dot(t_new.T).T.dot(q) #WORKS BUT IS THIS RIGHT?        \n",
    "    \n",
    "    #Set outer relation for the X block\n",
    "    #Xres_dictionary[i] = E_h #MAYBE REMOVE\n",
    "    X = E_h\n",
    "        \n",
    "    #Set the mixed relation for the Y block\n",
    "    #Yres_dictionary[i] = F_h 3MAYBE REMOVE\n",
    "    Y = F_h\n",
    "        \n",
    "    #Find estimated t hat\n",
    "    t_hat = np.dot(E_h,w_new.T)\n",
    "    t_hat_dictionary[i] = t_hat\n",
    "    E_h = E_h - np.dot(t_hat,p_new.T)\n",
    "    \n",
    "    return X,Y, u, w_new, q_new, t_new, p_new, q_new_dictionary, t_hat_dictionary, b_dictionary,E_h, F_h \n",
    "          \n",
    "\n",
    "def pls_optimized(path, path_test, predictors, response):\n",
    "    '''Function that takes a dataframe and runs partial least squares on numeric predictors for a numeric response.\n",
    "    Returns the residuals of the predictor (X block), response (Y block), and traininig RMSE'''\n",
    "    ###TRAINING DATA\n",
    "    combined = predictors\n",
    "    #Load data\n",
    "    data = pd.DataFrame.from_csv(path)\n",
    "    combined.append(response)\n",
    "    data = data[combined]\n",
    "    response_std = data[response].std()\n",
    "    \n",
    "    #Subtract the mean and scale each column\n",
    "    data = mean_center_scale(data)\n",
    "\n",
    "    #Separate in to design matrix (X block) and response column vector (Y block)\n",
    "    predictors.pop()\n",
    "    X = data[predictors].as_matrix()\n",
    "    Y = data[[response]].as_matrix()\n",
    "    Y_true = Y #For prediction\n",
    "    \n",
    "    #Get rank of matrix\n",
    "    rank = np.linalg.matrix_rank(X)\n",
    "   \n",
    "    u = Y #set initial u as Y\n",
    "    Xres_dictionary = {}\n",
    "    Yres_dictionary = {}\n",
    "    q_new_dictionary ={}\n",
    "    b_dictionary = {}\n",
    "    t_hat_dictionary = {}\n",
    "    t_hat_train_dictionary = {}\n",
    "    t_hat_test_dictionary = {}\n",
    "    RMSE_dictionary = {}\n",
    "    RMSE_test_dictionary = {}\n",
    "    \n",
    "    ###TEST DATA\n",
    "    #Load data\n",
    "    data_test = pd.DataFrame.from_csv(path_test)\n",
    "    combined.append(response)\n",
    "    data_test = data_test[combined]\n",
    "    response_std_test = data_test[response].std()\n",
    "    \n",
    "    #Subtract the mean and scale each column\n",
    "    data_test = mean_center_scale(data_test)\n",
    "\n",
    "    #Separate in to design matrix (X block) and response column vector (Y block)\n",
    "    predictors.pop()\n",
    "    X_test = data[predictors].as_matrix()\n",
    "    Y_test = data[[response]].as_matrix()\n",
    "    Y_true_test = Y_test #For prediction\n",
    "      \n",
    "    #Get rank of matrix\n",
    "    rank_test = np.linalg.matrix_rank(X_test)\n",
    "    \n",
    "    #Iterate through each component\n",
    "    for i in range(1,(rank+1)):\n",
    "        Y_pred = np.zeros((Y_true.shape[0],1))\n",
    "        Y_pred_test = np.zeros((Y_true_test.shape[0],1))\n",
    "        \n",
    "        #Core algo\n",
    "        X,Y, u, w_new, q_new, t_new, p_new, q_new_dictionary, t_hat_dictionary, b_dictionary,E_h, F_h = core_pls(i,Y, X, q_new_dictionary, b_dictionary, t_hat_dictionary)\n",
    "                \n",
    "        #NEW Sum over different compenents\n",
    "        for g in range(1,i+1):\n",
    "            t_hat_train = np.dot(E_h,w_new.T)\n",
    "            t_hat_train_dictionary[g] = t_hat_train\n",
    "            E_h = E_h - np.dot(t_hat_train, p_new.T)\n",
    "            Y_pred = y_pred(Y_pred, g,b_dictionary,t_hat_dictionary,q_new_dictionary)\n",
    "        \n",
    "        #Find training RMSE \n",
    "        RMSE_dictionary = rmse(i,Y_true, Y_pred, response_std, RMSE_dictionary)\n",
    "        \n",
    "        #Set initial E_h as X_test data\n",
    "        E_h_test = X_test\n",
    "        \n",
    "        #Sum over different compenents\n",
    "        for k in range(1,i+1):\n",
    "            t_hat_test = np.dot(E_h_test,w_new.T)\n",
    "            t_hat_test_dictionary[k] = t_hat_test\n",
    "            E_h_test = E_h_test - np.dot(t_hat_test, p_new.T)\n",
    "            Y_pred_test = y_pred(Y_pred_test, k,b_dictionary,t_hat_test_dictionary,q_new_dictionary)\n",
    "        \n",
    "        #Find test RMSE \n",
    "        RMSE_test_dictionary = rmse(i,Y_true_test, Y_pred_test, response_std_test, RMSE_test_dictionary)\n",
    "        \n",
    "    return RMSE_dictionary, RMSE_test_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications to simulated data sets\n",
    "\n",
    "Since the PLS method is especially beneficial to use when one is performing regressions on a dataset with 1) more predictors than observations, and 2) highly correlated variables, we decided to simulate a dataset that played to these strengths. Our simulated data contained 11 predictors (x1-x11), where x4-x11 were functions of the first three variables (i.e., they were correlated) and one response variable (y) that was only a function of two of the predictors. Additionally, our training and test dataset only contained 10 observations each, so one value less than the number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate dataset with 11 predictors (x1-x11) and a response variable y, with only 10 observations in each dataset.\n",
    "# variables are made to be correlated with one another and response variable doesn't rely on all variables. \n",
    "np.random.seed(9856)\n",
    "x1 = np.random.normal(5, .2, 20)\n",
    "x2 = np.random.normal(7, .4, 20)\n",
    "x3 = np.random.normal(9, .8, 20)\n",
    "\n",
    "sim_data = {'x1' : x1,\n",
    "            'x2' : x2, \n",
    "            'x3' : x3,\n",
    "            'x4' : 5*x1,\n",
    "            'x5' : 2*x2,\n",
    "            'x6' : 4*x3,\n",
    "            'x7' : 6*x1,\n",
    "            'x8' : 5*x2,\n",
    "            'x9' : 4*x3,\n",
    "            'x10' : 2*x1,\n",
    "            'x11' : 3*x2,\n",
    "            'y' : 6*x1 + 3*x2}\n",
    "\n",
    "# convert data to csv file\n",
    "pd.DataFrame(sim_data)[0:10].to_csv(\"sim_data_train.csv\")\n",
    "pd.DataFrame(sim_data)[10:20].to_csv(\"sim_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set variables for input to pls function\n",
    "sim_predictors = pd.DataFrame(sim_data).drop(\"y\", axis = 1).columns.tolist()\n",
    "sim_response = \"y\"\n",
    "sim_data_path = 'sim_data_train.csv'\n",
    "sim_data_test_path = 'sim_data_test.csv'\n",
    "\n",
    "# run pls regression on simulated data\n",
    "pls_sim_results = pls(sim_data_path, sim_data_test_path, sim_predictors, sim_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run pls optimized regression on simulated data\n",
    "pls_optimized_sim_results = pls_optimized(sim_data_path, sim_data_test_path, sim_predictors, sim_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications to real data sets\n",
    "\n",
    "In the paper we selected, the authors don’t include real-world examples with data that we can test on. Thus, we decided to test the algorithm on a publicly available dataset that we investigated for our predictive modeling class. This data contains information on residential properties in Ames, Iowa, with variables that describe characteristics of each home and a variable indicating the home’s value.$^5$\n",
    "\n",
    "We decided to use this dataset because it contains many correlated variables, some of which have very similar meanings. Additionally, since there are several different factor/categorical variables in the dataset, the total number of predictors increases when trying to estimate a coefficient for each level. With this data, we were trying to predict a home’s price based on its attributes. The limitations of the data, including the number of predictors and collinearity, suggest that the PLS method will be beneficial for predictions and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set variables for input to pls function\n",
    "predictors = x_predictors_train.columns.tolist()\n",
    "response = \"price\"\n",
    "path = 'ames_data_train.csv'\n",
    "path_test = 'ames_data_test.csv'\n",
    "\n",
    "# run regression using pls code and optimized pls code on ames housing data\n",
    "pls_reg_results = pls(path, path_test, predictors, response)\n",
    "pls_reg_opt_results = pls_optimized(path, path_test, predictors, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find speed of initial pls function\n",
    "pls_reg_speed = %timeit -o -q pls(path, path_test, predictors, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find speed of initial pls function\n",
    "pls_reg_opt_speed = %timeit -o -q pls_optimized(path, path_test, predictors, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to get list of values from dictionary input\n",
    "def get_list(input_dict):\n",
    "    '''input dictionary with key/values, and then output the list of values'''\n",
    "    length = len(input_dict)\n",
    "    x = list(range(1, length + 1 , 1))\n",
    "    \n",
    "    mykeys = input_dict.keys() # get keys for dictionary\n",
    "    value_list =[input_dict[x] for x in mykeys] # get list of values from dictioanry\n",
    "    value_list = [ int(x) for x in value_list ]\n",
    "    return value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGHCAYAAABbKOOPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrMX9oD5GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9430db87f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get list of rmse values from pls regression on ames data\n",
    "pls_rmse_list = get_list(pls_reg_results[0])\n",
    "\n",
    "# plot rmse values from pls regression on ames data\n",
    "plt.plot(pls_rmse_list)\n",
    "plt.title(\"PLS: RMSE by # of Principal Components\")\n",
    "plt.xlabel(\"# Principal Components in Regression\")\n",
    "plt.ylabel(\"Root-Mean-Square Error (RMSE)\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get minimum rmse values using the pls method and the optimized pls method\n",
    "pls_rmse_test_list = get_list(pls_reg_results[1])\n",
    "pls_opt_rmse_test_list = get_list(pls_reg_opt_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PLS</th>\n",
       "      <td>46178</td>\n",
       "      <td>0.444151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLS Optimized</th>\n",
       "      <td>46178</td>\n",
       "      <td>0.242239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                RMSE     Speed\n",
       "PLS            46178  0.444151\n",
       "PLS Optimized  46178  0.242239"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare accuracy and speed for the pls method (optimized and not) \n",
    "pls_comparison = {'RMSE' : [min(pls_rmse_test_list), min(pls_opt_rmse_test_list)], 'Speed' : [pls_reg_speed.best, pls_reg_opt_speed.best]}\n",
    "pd.DataFrame(pls_comparison, index=['PLS', 'PLS Optimized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative analysis with competing algorithms\n",
    "\n",
    "With the Ames housing data, we decided to test our PLS results above with the standard OLS regression and the PCR method. In the OLS regression, we tried predicting price using all of the variables in the dataset. The model is as follows:\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "$$y = \\beta_0 + x_1\\beta_1 + ... + x_p\\beta_p + \\epsilon$$ \n",
    "$$y = \\beta_0 + area^*\\beta_{area} + ... + Lot.Config^*\\beta_{Lot.Config} + \\epsilon$$ \n",
    "where we have $p$ predictors available in the data.\n",
    "\n",
    "For the OLS model, $\\beta$ is estimated by minimizing the sum of squared errors and can be express as $(X^T X)^{-1} X^TY$. All predictors are used in estimating the $\\beta$ coefficients. The PCR regression is similar to the PLS regression in that it selects a subset of features out of the larger number of predictors to include in the model. In the PCR model, the predictors are first scaled and then we perform 10-fold cross-validation on the model with varying numbers of predictors (from 0, which is a model with just the intercept, up to the total number of predictors). We find the model with the smallest RMSE to identify the number of principal components to be used in the final model (see the plot below for a visualization). Using that set of predictors, we predict the house price on the test data and calculate the RMSE.\n",
    "\n",
    "In implementing the PCR algorithm below, we loosely followed an example conducted by Professor Crouser at Smith College on baseball data.$^6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Algorithm: OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to run the ols regression on a set of input training and test data\n",
    "def run_ols_reg(x_train, y_train, x_test, y_test):\n",
    "    '''Run ordinary least squares regression on set of x predictors and y response variable.\n",
    "    Fit on training data and predict on test data. Output RMSE of model.'''\n",
    "\n",
    "    # run ols on training data\n",
    "    ols_reg = lin_reg().fit(x_train, y_train)\n",
    "    \n",
    "    # predict on test data and get rmse estimate\n",
    "    y_test_pred_ols = ols_reg.predict(x_test)\n",
    "    ols_rmse_test = sqrt(mean_squared_error(y_test, y_test_pred_ols))\n",
    "    \n",
    "    return ols_rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Algorithm: PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create function to do k-fold cross validation on the input data\n",
    "def cross_val(x_data, y_data, num_folds):\n",
    "    '''Perform k-fold cross-validation on training data. Output list of RMSE values'''\n",
    "    \n",
    "    # cross validation (k-fold)\n",
    "    n = len(x_data)\n",
    "    cross_val_k_10 = cross_validation.KFold(n, n_folds = num_folds, shuffle = True, random_state = 1)\n",
    "    rmse = []\n",
    "\n",
    "    # calculate rmse values for intercept (no predictors/components)\n",
    "    mean_sq_error = -1*cross_validation.cross_val_score(lin_reg(), np.ones((n,1)), y_data.ravel(), cv = cross_val_k_10, scoring = \"neg_mean_squared_error\").mean()\n",
    "    rmse.append(sqrt(mean_sq_error))\n",
    "    \n",
    "    # calculate rmse values for varying numbers of predictors/components\n",
    "    for i in range(1, x_data.shape[1]):\n",
    "        mean_sq_error = -1*cross_validation.cross_val_score(lin_reg(), x_data[:,:i], y_data.ravel(), cv = cross_val_k_10, scoring = \"neg_mean_squared_error\").mean()\n",
    "        rmse.append(sqrt(mean_sq_error))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_pcr_reg(x_train, y_train, x_test, y_test):\n",
    "    '''Run ordinary least squares regression on set of x predictors and y response variable.\n",
    "    Fit on training data and predict on test data. Output RMSE (both training and test data).'''\n",
    "\n",
    "    # scale x train data\n",
    "    x_scaled_train = PCA().fit_transform(scale(x_train))\n",
    "\n",
    "    # perform cross-validation on the x scaled data\n",
    "    pcr_rmse = cross_val(x_scaled_train, y_train, 10)\n",
    "\n",
    "    # find number of principal components with smallest rmse\n",
    "    n_pcr_comps = pcr_rmse.index(min(pcr_rmse))\n",
    "\n",
    "    # run regression using specific number of principal components\n",
    "    pcr_reg = lin_reg().fit(x_scaled_train[:,:n_pcr_comps], y_train)\n",
    "\n",
    "    # use regression above to predict on test data\n",
    "    x_scaled_test = PCA().fit_transform(scale(x_test))[:,:n_pcr_comps]\n",
    "    y_test_pred_pcr = pcr_reg.predict(x_scaled_test)\n",
    "    pcr_rmse_test = sqrt(mean_squared_error(y_test, y_test_pred_pcr))\n",
    "    \n",
    "    # output rmse train/test values for pcr regression\n",
    "    return pcr_rmse, pcr_rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run ols regression on ames data\n",
    "ols_reg_results = run_ols_reg(x_predictors_train, y_train, x_predictors_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get speed of ols regression on ames data\n",
    "ols_speed = %timeit -o -q run_ols_reg(x_predictors_train, y_train, x_predictors_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run pcr regression on ames data\n",
    "pcr_reg_results = run_pcr_reg(x_predictors_train, y_train, x_predictors_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get speed of pcr regression on ames data\n",
    "pcr_speed = %timeit -o -q run_pcr_reg(x_predictors_train, y_train, x_predictors_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PLS</th>\n",
       "      <td>46178.000000</td>\n",
       "      <td>0.444151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLS Optimized</th>\n",
       "      <td>46178.000000</td>\n",
       "      <td>0.242239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>67014.500338</td>\n",
       "      <td>0.502869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>40592.231387</td>\n",
       "      <td>0.001790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       RMSE     Speed\n",
       "PLS            46178.000000  0.444151\n",
       "PLS Optimized  46178.000000  0.242239\n",
       "PCR            67014.500338  0.502869\n",
       "OLS            40592.231387  0.001790"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of all methods, accuracy and speed\n",
    "diff_methods_summary = {'RMSE' : [min(pls_rmse_test_list), min(pls_opt_rmse_test_list), pcr_reg_results[1], ols_reg_results], 'Speed' : [pls_reg_speed.best, pls_reg_opt_speed.best, pcr_speed.best, ols_speed.best]}\n",
    "pd.DataFrame(diff_methods_summary, index=['PLS', 'PLS Optimized', 'PCR', 'OLS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAGHCAYAAAD/QltcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xl8ZFWZ//HPt1kaukkaZEdBQFY3oBtQRhQVEdGfC4pL\nAFncBkHFdmYEZRTRUdywFQcElXWQKLiBiqAsriCabhCQVTZZGwjQNL3R3Xl+f5xbpFJdSere2pPv\n+/WqV1K37vLk1k3y1DnPPUcRgZmZmVkrTGl3AGZmZjZ5OPEwMzOzlnHiYWZmZi3jxMPMzMxaxomH\nmZmZtYwTDzMzM2sZJx5mZmbWMk48zMzMrGWceJiZmVnLOPEwayJJz5c0JOkT7Y6lGSRtJOnHkh6T\ntFLSx5p8vCFJn23i/n8n6apm7T87xuckDTXzGGadzInHBCfp0OyPdemxRNJtkr4taaMq628k6euS\nbpG0SNLTkgYkHSdpRtl6v6vY72JJf5d0tCTVEe/zK/a7UtKgpEskvbzK+seXrffcKq/3ZD/zkKST\nK17bQNK3sp91saT5kq6V9GVJ08rWO6siphE/d9GftZkkvTSLb9vs+WxJdzfhUN8E9gG+CLwXuHSM\nmCrf1wckXSZprxzHi+zRLAE0OynI9TNIerWkn0p6SNKy7Dq9WNL+TYxx0pD0KUlvbXcck8nq7Q7A\nWiKAzwD3AGsBewIfBvaT9OKIWAogaTfgEmAacB4wN9t+V+AY4JXAG8r2eR9wLCBgA+BAYE72/Wfq\njPn8LJbVgO2Ao4ArJe0WEf+osv5SoA/4esXyt1PlD72k9Ug/3zrAmcCtwPrAS4EjgFOBf1Xs//2k\nn7Xcyrw/WIu8DBiMiDuy5y8HrmnCcV4D/Dwi5tS4/m+Ac0nncSvgSNL7+saIuKyG7dcGVhSKtDb7\nNHHfuUk6gfS7dDtwGnAv6Tp9I/BjSQdFxA/bGOJE8GngQuCidgcyWTjxmDwujYh52fdnSnocmA28\nFfhR1prxM2A5sHPZPyyA70o6DvhgxT4XRER/6Ymk00n/wD8q6bNR3wyE8yLi/LJ9/wn4NSlh+kjF\nukFKUqolHgcCvwQOqFj+AeB5wL9FxLXlL0haB3imYv0V5T9rF9gd+GvZ8z1Y9dw0wkbAghzr317x\nvv4cuAH4OFA18cha0NaMiGURUfm+NFRENDOpyUXSAaSk4wLgoIgoT3JPkrQPsEZbgjOrg7taJq8r\nGf7UCelT/qbA7IqkA4CIeDQivjTWDiNiGfA3oIf0D+lZkjaXtH0d8f4x+/qCUV4/H9hF0nZlx9wY\neG32WqWtgZWVSQdARDzdjH9wkj4u6Z6sW+d3kl5U9tphWRfETlW2+7SkFZI2HWf/60paX9IGpBaP\nm7LnLyIlWf/Mnk+vIdatJF2YdXMtknSNpDeWvX5oWZ3CR0rdJzWeimdFxE3AYwxfh6UumZMlHSjp\nJlJr075lr322bN3PZcteIOlsSU9IelLSmZLWqvJzHZx1py2S9Lik30t6Xdnrv5N0ZdnzvbL9v0vS\nl7LujqclXSTpeRX73lPSBZLulbRU0r8kfaNaHDX6AjAIvL8i6Sidu99GxCVlx99Q0hmSHlbqXrxe\n0iEVMT5bcyTpSEl3ZufiMmVdlZI+I+m+7Dr9uaR1K/ZxT9bVs4+k67Jj/aNa189411G2Tukcv1Op\nS/e+bJ+XS1rl913SyyRdmr3Pi7L37N8q1qnpusiu4WlA6fdvSNKZ2WvrSPqmpLuz93O+pN9I2nms\nN83G5xaPyWub7Otj2de3AEuAn9S5361ILRBPViz/P+BVFE92S/+Ynhjl9T8A95NaOD6XLXsPsBD4\nVZX17wVWl3RIRJxbSwCS1q+y+JmIWFjD5oeSunX+l9TddTRwhaSXRMSjwI+BU4CDgL9XbHsgcGVE\nPDTOMa4Dnl/2/EXAf2XfB6nlJ4BzgPeNthOl2p9rsji/BTyexX+xpHdExEXA74GDSV1ype6T3JS6\nvNYDKpPdvYF3kc7XY6RuwmpKrWoXAHeRuv5mklq05gOfKjvW8cDxwJ9JLQnPkBK01wKXV+yv0nGk\n2o8vk5Lq2cBvJe2cJdwA7yR1BZ1KShh2Bz4KPBd49yj7rUrSNsD2wPcjYlEN669Fek+2Br5NOl/v\nBM6WNCMivl2xycGk1pKTgeeQulIvzJKuvbKfcxvgY6SWsg+UbRuk7s8fkrp/zgYOz7bfNyKuyGKq\n5Toqdyyp6/JrwIwspvNIrXWln/O1pNbNAdLv+VB27Csl7RkRA2UxwvjXxcHAGcC1wHezZXdmX08n\nddV+G7iF1MW1J7AjcD1WXET4MYEfpF/0laS++PUZ/iP4KPA0sGm23iCpe6PW/V4F/CPb5/qkP0Rf\nJf0huGiU9VfUsN/nZ/v472y/G5F+2f+a/Rz7V6x/fLb8Odnxbyt77Vrge9n3Q8DJZa9tRPoDNATc\nTPpn8R6gt0pMZ2XrVXtcUuPP8zSwSdny3bLlXy9b9gPgvortd8nWe28N524P0j/RE4BlpHqF15IS\nr2uza+C1wA7j7GdOdk73KFs2nfQH+c6KdUec13H2O0T6474+qQ5od9I//JXA0RXrLQe2H2Ufn614\n/4eA71as9xPgkbLnLyDVhlxYw3V9ZdnzvbL9/wuYVrb8gGz5R8qWTa2yv2Oy4z6v8podJ443Z/v/\nWI3n9ujsPL6nbNlqpCRrATC94np8GFinbN0vZsvnAVMqrsklwBply+7OjvXWsmU9wAPAQN7rqOwc\n3wSsVrb8o9n2Lyxbdhvwq4qffWq2z0vzXhfZsoXAmVXO6RO1Xtt+5HtMuq4WSa/MmgkfyJrV3pJz\n+6lKdzncIGm5pJ9WWad0F8RKjazkv7FxP0kuAq4gJRv3kboengLeFsOfontJv4B57Jjt81FSbcd/\nkgq0Dq9cMSJeExF5WthOyPb7MKk1Y3vgExHxszG2OR/YVtKsrIl2N6p3sxARj5AKSb8DrAv8e7bu\nI5L+u8omS0ifwl9X8Ti2xp/nZxHxcNnx/0ZKBsqbnc8FNpP0mrJlBwGLgVWusyo/0zURcSWpZeVv\nkZrirwS2AH4REVdFxJURces4u9oP+GtEPFuMGulT93eBLSW9cLxYxvB+0vv6CPAXUrJ0UkR8q2K9\n30XEbTXuM0ifTsv9EVhfqV4HYH/S78HnC0UN50TEs3cwRcSPgYcoe/9iuOUDSdOyFrJrSK18u+Q8\nXm/2tdbfyf2Ah6Os0DRS98zJpOuh8s6hCyLi6bLnpS7H/4uIoYrla5I+sJR7MMpaLCK1+p1L6u4s\ndbPmvY7OjJFdSn8kvWdbA2RdHNsC/Updhutn57iH9PftVRX7q+W6GMuTwMs0Then5TcZu1qmk5rJ\nzqCGP+ZVrEb6R/At4B2jrPMx0iedktVJBXQXFDheIwTp7oE7SJ++5lf5o/4U6Rc4j7tJTZerkT5R\nHgdsSOqTr9d3SZXma5E+pX+Mca7XiLhe0q2krokFwEMRMeqYDBExn3S3zFFZ0/a+pPftBEkPRsSZ\nZauvHGtfNfhnlWW3k5rDS35LSrQOAq6SJFIrzM9jnOZ2Sb2kpnOREqQrsj/K65O6XG7Ini+PiKfG\nifX5pKSg0i1lr988zj5GcxGp+yRI/1T/ERFLqqx3T879/qviealLbj1Sa9PWpE/At1BMtffvn8CW\npSeSNifVZbw5O25JkLoO8ii9R7X+Tj6fVburIP28YmQXHKQPIOVKBcL3j7J8PUa+J6Ndz5DOySPk\nv44qYyp/DyElHTB6t95Q1q1UXuw83nUxlk+SupHukzSX1MVzbkQ047b0SWXSJR4RcSnZWAPZH/YR\nJK0JfIn0B39d4Ebg2Ij4fbb9YtI/KyTtSZU/KFn2v7Bsn2/L9nV2Y3+aXP4Ww3e1VHMrsJOk1aP2\nyv5FZf+ML5d0Namp9kukuxTqcUf2aR3gkqwI7CuSrhrn5zifdOfLQuBHtR4sIv5JKr68hPQH/CDS\nbbYtExFDks4HPiDpSNLty5uR+rnHcxEjP9W+hFSHAOkf38+z739HSuTa5f6y93Us1ZKRsYxW2Fp4\nTJk8JE0hdRutC5xI6hJYRGopOIf8tU2lVqmXNCrGCqOdr3aex/GOXTqH/8GqdVAllclE4Z8nIi6U\n9AdSa9nrSS26x0jaP2q79dtGMem6WmpwCqng7F2kX/oLgV9Xq67O4X3A5RFRmdF3kl+QCuNGa8UZ\nV0TcSPon+e+qqPhvgC+Skon/GWe980l352zLKN0sY8k+zTyR7aORtq2ybDtW/WR/LqmZ/c2klptH\nSMWb4/kEqevns6RWrTdkzy8mjVdS6ib6jxr2dS+pa6vSjmWvd5s7SX/vinYTVXv/tmH4/XtJts4n\nIuLrEfGLLMEaryC4qkh3lt0GvFVlg9mN4d5RYmzWe7ZNlWWla+aesmM28joqFX0uzLoMqz2KjKsz\n6m3/ETE/Ik6LiLeTCtwHSS27VgcnHmWyptLDgHdGxNURcXdEfINUoLVK3UKN+9yU1Nf5vYYF2hyn\nkZr5T1I22mU5pRFNa/mF+yqpT3jEEOGq83barPn0dGBfSS8dY727SIV2n4rhCvdVSNq92h90SbuT\nuifGq4PI622SNqs4zstIzbfPypK3G0ljprwD6K/oc68qIq7L/tGtDtxUVt+xMSnpLdV3XFdDrJcA\nu0t6WVm804EPAXdHRNFulnb6OekfzGertXTW4JDyugBJ7yQlp6X3r/QPr/Jv6scpPtLq8aQi3DMk\nrVb5otLtrG/Knl4CbCLp3WWvr0Yq0FxIuuOlkTZT2e2zWVffe4HrsvqpUkyNvI7mkpKP/1SVW8KV\nbiMvYhGppap8X1Oyn+lZEfEY8CCpmNXqMOm6WsbxElK9wu0Vf5zWZPi207wOI32CbueoeLU0Kz6Z\n/SH5FXC9pPKRS2eSBue6uob93JJ1V3xA0hciotSnWu/ttJDqaj5OKug8cIwYKm8drOa9wEGSfkb6\nOZ8hfRo+nNTMf2LF+qtLOmiUff10lDqFcv8E/iTpOwzfTvso6dbBSueSbmEM0l0FebyC7H3KbrHc\nhdRalMeXSe/3pUrDzD9Ouo6fT7q9sOtExJ2Svki6W+qPWVH4MlIB8gMRMV5S/Tjp/TsL2IT0/t0O\nfD97/VbSP8WTsta+p0iJ47pV9lVrzBdIeglpZM1dJPUzPHLpG0hdZqXfg++SCqTPlrQrw7fT7kG6\nY2jcW3LHUO3vx+3A95VGO55PKhreiHQXXUlDr6OICEkfICU0/8jeiwdI3VmvIdWjFBn6fC7wOkmz\nSYnF3aTWpvsl/ZjUrfM06S6xXan4UGX5OfEYaR1SM/VMVp2vYbxCpNEcTipIaueIiDV94oqIv0p6\nMWnshzeR7nEfIv2R+QrpfvZa9vs1UrX/Rxm+iyDPHBhV57KIiIeyGoiDJR2Xs8ircp+nkT7p7E0a\nw6SXlAhcCnw5Iir7kKcyelHbH1m1iK3y2OdkXz9O+gN9LfDRrMC10g9I5/uOsVptKmV1BrszXJsy\nk1Rwmmuo9Ih4RNIeWQwfISVKNwD/L6uRGrE6tX+ir3XdsdbLc7yRG0YcL+ku0nX5P6Qi8RtY9X2t\n3H+Q6pZeSkp6e0iFwEdFNt1ARKyQ9P9Id5EcSyqw/imp67ZaPUKtv5OfkXQFqbj6CNJt40+SBurb\nPyIuztZbqjTnzZeBQ0jX823AYRHxf1WOXe34Y53zSneQzuPXSV2GdwPviojLn90o/3U07rEj4vfZ\nPj9DqrVbh9RSey2r3sFSq09k236B1N18DqlV5hRSbcf+pA9M/wQ+HBHfHWU/ViNFFG0F7H5ZweLb\nSr+8WRfDrcCrIuLPNWx/FjAj6/+r9vqrSbd5vTgiilbT2yST3X3yEPC5GGe0WGuu7J/5VcABEVHk\nLrgJR2mywRsjItdQBGYlHVHjoQJja0haU9IXlYbvXSrpLkmH1bDddEk7aXjY262z55tnBV3nA+dK\n2l/SllktwLGS9ivbx47Z9s8BZmTbrzLUNan58VonHZbT4aTfzVruZjEz6yqd0tVSZGyNC0ljRhxO\n6lvdlNoSqV1Jn2BKzY0nZctLw0gfRuoH/jqp7/Ax0r3ovyjbxyWkQZlKrsv29WwBWFaYtD+pidRs\nXEoDh72I1Kf/s4gYq/vGzKwrdUTiMd7YGpUkvYE0xsHWEVGaE6SmP9LZeByjJijZ7VgnZI/R1tlq\ntNfK1nmK1P9oVqvPkooB/4QT1k4yefujqytcZ2MGHVjjUVl3Mco6p5DuWZ9LujthEWm8gs+Uir3M\nzMys83REi0cBW5NaPJYCbyPd6/4dUs3F+9sYl5mZmY2hWxOPKaRbMw8sTXQk6ROkaZmPjLLJmkqy\nOwX2Jd3f7lYRMzOz2q1FmofnsogYrGdH3Zp4PEQa9Kd8bI3SZEjPY3ho3XL7kn8wJjMzMxt2EAWm\noyjXrYnHn4EDJE2L4amqtye1glTOrlhyD8B5553HjjvuOMoqVmn27NnMmTOn3WF0HZ+3/HzOivF5\ny8/nLL9bbrmFgw8+GPLPHL2Kjkg8snH3t2F4aN6ts3ExHo+I+ySdCGwWEaXheM8n3fJ6lqTPkW6r\n/SpwRrVulsxSgB133JGZM2c26SeZeGbMmOHzVYDPW34+Z8X4vOXnc1aXuksVOmIAMdLYGteR7lIp\nja0xj+FbWjcBNi+tnM07sA9pHoS/keYBuYg0f4KZmZl1qI5o8ahhbI1VZoaNiNtJdRtmZmbWJTql\nxcPMzMwmASceNqa+vr52h9CVfN7y8zkrxuctP5+z9uq4kUubRdJMYO7cuXNdVGRmZpbDvHnzmDVr\nFsCsiJhXz77c4mFmZmYt48TDzMzMWsaJh5mZmbWMEw8zMzNrGSceZmZm1jJOPMzMzKxlnHiYmZlZ\nyzjxMDMzs5Zx4mFmZmYt48TDzMzMWsaJh5mZmbWMEw8zMzNrGSceZmZmNbr3XhgaancU3c2Jh5mZ\nWQ3mz4dtt4Xf/KbdkXQ3Jx5mZmY1+NvfYPlyeOihdkfS3Zx4mJmZ1WBgIH1duLC9cXQ7Jx5mZmY1\nmDs3fX3qqfbG0e2ceJiZmY0jYrjFw4lHfZx4mJmZjePBB+Hhh9P3Tjzq48TDzMxsHKXWjhe8wIlH\nvZx4mJmZjWPuXNhoI3jhC11cWi8nHmZmZuMYGIBdd4UZM9ziUS8nHmZmZmMoFZbuuiv09jrxqJcT\nDzMzszHcfz88+ijMmgU9PU486uXEw8zMbAylwlK3eDRGRyQekl4p6WJJD0gakvSWHNu+QtJySfOa\nGaOZmU1OAwOw6aaw2WYp8XBxaX06IvEApgPXA0cCUetGkmYA5wCXNykuMzOb5ObOTd0skBKPZcvS\nw4rpiMQjIi6NiM9GxEWAcmx6GvAD4C/NiczMzCaz8sJSSIkHuNWjHh2ReBQh6XBgK+CEdsdiZmYT\n0733wuDgcOLR05O+us6juNXbHUARkrYFvgTsGRFDUp5GEjMzs9qUJoYr72oBJx716LrEQ9IUUvfK\n8RFxZ2lxrdvPnj2bGTNmjFjW19dHX19f44I0M7MJYWAAnvtc2GST9HwydLX09/fT398/YtmCBQsa\ntn9F1FzL2RKShoC3RcTFo7w+A3gCWMFwwjEl+34F8PqI+F2V7WYCc+fOncvMmTObEbqZmU0w++wD\n06fDz3+enj/0ULq75Ze/hDe9qb2xtdK8efOYlZp9ZkVEXXeRdmONx1PAi4GdgZ2yx2nArdn317Yv\nNDMzmygiUldLqb4D3NXSCB3R1SJpOrANwy0YW0vaCXg8Iu6TdCKwWUQcGqmJ5uaK7R8BlkbELS0N\n3MzMJqy774Ynnhiu7wCYNg2mTHHiUY+OSDyAXYGrSGN4BHBStvwc4H3AJsDm7QnNzMwmo9KIpeWJ\nh+Rh0+vVEYlHRPyeMbp9IuLwcbY/Ad9Wa2ZmDTQwAFtsARttNHK5Ry+tTzfWeJiZmTVd+Yil5Txf\nS33qSjwkTW1UIGZmZp1iaGjVwtISJx71yZV4SNpP0jmS7pK0HFgs6SlJv5d0nKTNmhSnmZlZy9x5\nJyxYUD3xcI1HfWpKPCTtL+l24EzSWBlfAd4O7At8APg98DrgLkmnSdqwSfGamZk1XeWIpeXc4lGf\nWotLPwnMBn4dEUNVXr8AQNJzgY8CBwNzGhKhmZlZiw0MwJZbwvrrr/paby/cf3/LQ5owako8ImKP\nGtd7ADi2rojMzMzarHxG2kpu8aiP72oxMzMrMzQE8+Y58WiWmhMPSTdLek7Z81MlbVD2fCNJixsd\noJmZWSvdcUcap6NafQe4uLReeVo8dmBk18zBQG/ZcwFrNSIoMzOzdqk2Ymm50gBiHTbHateop6ul\n2lT0fhvMzKyrDQzAC14A661X/fXe3pR0LFrU2rgmCtd4mJmZlRltxNISz1BbnzyJR2kCt8plZmZm\nE8LKlWMXloITj3rlmSROwBWSVmTP1wZ+IemZAvsyMzPrOLfdlrpQxko8enrSVycexeRJFipnf72o\nyjo/qSMWMzOztiqNWDpz5ujruMWjPjUnHtnU82ZmZhPWwABsuy3MmDH6OqXEY+HC1sQ00dTdPSJp\nL2A6cE1EPFF/SGZmZu0x1oilJe5qqU+eAcSOkfSFsueSdClwFfBL4BZJL2pCjGZmZk23YgVcf/34\niceaa8JaaznxKCrPXS3vBm4qe34A8CrglcAGwABwfONCMzMza51bb4XFi8e+lbbEo5cWlyfx2Aq4\noez5G4EfR8SfI+Jx4H+AmiaTMzMz6zQDAyDBLruMv67naykuT+KxOrCs7PkewNVlzx8ktXyYmZl1\nnblzYfvth4tHx1IaNt3yy5N43EnqWkHSFsB2wB/KXn8eMNi40MzMzFpnYKC2bhZwi0c98iQepwD/\nK+kM4Neku1huLnv9tcB1jQzOzMysFZYvr62wtMQ1HsXVnHhExPeAjwHPIbV0vKNilc2AMxsXmpmZ\nWWvcfDMsXVp74uEWj+JyjeMREWcySnIREUc2JCIzM7MWmzsXpkyBnXeubf3eXrjllubGNFF5dloz\nM5v0BgZghx1gnXVqW9/FpcXV3OIhaWUt60XEasXDMTMza71aRiwt566W4vLOTnsvcA4uIjUzswni\nmWfghhvgve+tfRsXlxaXp6tld+BS4GjSCKWbA3+IiIvKH0WCkPRKSRdLekDSkKS3jLP+/pJ+I+kR\nSQskXS3p9UWObWZmk9s//gHLltV+Ky2kFo+lS1PSYvnkuatlICI+DGwKfAPYH7hf0g8l7VNnHNOB\n64Ejgahh/VcBvwH2A2aS5ov5haSd6ozDzMwmmYGBfIWl4Blq65F7dtqIWAqcB5wnaSvgDOBSSRtm\nQ6fnFhGXklpTkKQa1p9dseg4SW8F3gz8vUgMZmY2Oc2dCy96EUybVvs25YnH+us3J66JKnfiASDp\necBh2WMa8DWgbb1dWbLSAxRKfMzMbPLKW1gKw4mH6zzyq7mrRdKakt4t6TfAHaQujo8Dm0fEsRGx\nollB1uC/SN01F7QxBjMz6zLLlqXC0jz1HZCKS8GJRxF5WjweAhaS7mo5EngkWz69vHckIlr6Nkg6\nEPgM8JaIeKyVxzYzs+52001puHS3eLROnsRjvezxGeC/q7wuUmFoy8bxkPQe4LvAARFxVS3bzJ49\nmxkzZoxY1tfXR19fXxMiNDOzTjYwAKuvDi99ab7tJnLi0d/fT39//4hlCxYsaNj+8yQer2nYURtA\nUh/wfeDdWXFqTebMmcPMmTObF5iZmXWNgYFUWLr22vm2mz4dpIl5V0u1D+Pz5s1jVt7+qFHUnHhE\nxO8bcsQqJE0HtiG1mgBsnd0a+3hE3CfpRGCziDg0W/9A4GzSpHV/k7Rxtt2SVnf1mJlZ9ypSWArp\n9lsPIlZMw+ZqkTRT0i8Lbr4raTTUuaTumpOAecAJ2eubkAYsK/kgqUvnFODBssc3Cx7fzMwmmaVL\nU41HkcQDnHgUlet2Wkn7AvsAzwDfj4i7JO0AfJk0hsZlRYLIWlNGTYIi4vCK5x3V7WNmZt3nhhtg\nxYr8d7SUeL6WYvJMEvd+4HvAIPAc4AOSPgF8G/gR8OKI8CTBZmbWFQYGYI018heWljjxKCZPV8vR\nwDERsSHwLmAD0m21L4mII5x0mJlZN5k7F17yEpg6tdj2vb0Ts7i02fIkHi8ALsy+/ymwAviviLi/\n4VGZmZk1WdHC0hK3eBSTJ/FYG1gMEBEBLCMNKmZmZtZVlixJs9LWc4eoi0uLyTtXywckPV227WGS\nRowWGhEnNyQyMzOzJvn732HlSrd4tEOexONfpNtYSx4G3luxTgBOPMzMrKMNDMCaa8KLX1x8H048\niskzgNiWTYzDzMysZQYG0t0sa65ZfB8uLi2mYQOImZmZdYu5c+vrZoHhFo+IxsQ0WdSUeGSTsdVE\n0uaSXlE8JDMzs+ZZtAhuvrn+xKOnB4aGYPHixsQ1WdTa4vFhSbdI+qSkHStflDRD0hslnU8a6nz9\nhkZpZmbWINdfnxKGRrR4gOs88qqpxiMi9pL0FuCjwImSFgHzgaXAeqS5VB4jTdz24oiY35xwzczM\n6jMwkAYNe+EL69tPKfFYuBA23bT+uCaLPMWlFwMXS9oA2BN4Pmlsj8dIE7xdFxFDTYnSzMysQebO\nhZ13TsOl18MtHsXkHceDiHgM+HkTYjEzM2u6gQF47Wvr348Tj2J8V4uZmU0aCxfCrbfWN2JpSU9P\n+urEIx8nHmZmNmlcf326/bXewlJwi0dRTjzMzGzSGBiAtdeGHVe5PzO/qVPTAGQeRCwfJx5mZjZp\n3HYb7LADrJ67wrE6D5ueX67EQ9Iaku6sNpaHmZlZpxschA03bNz+nHjklyvxiIjlwFpNisXMzKyp\nBgfhOc9p3P56epx45FWkq+UU4BhJDWqoMjMza43BQVi/gWNru8UjvyLJw27A3sDrJd0ILCp/MSLe\n3ojAzMzMGu3xxxufeLi4NJ8iiceTwE8aHYiZmVmzNaPF4+GHG7e/yaDIyKWHNyMQMzOzZlqyJD0a\nWePR2wu33964/U0Ghes0JG0IbJ89vS0iHm1MSGZmZo03OJi+NrLFw8Wl+eUuLpU0XdKZwEPAH7LH\ng5LOkDSCzsXHAAAgAElEQVSt0QGamZk1QjMSDxeX5lfkrpZvAHsBbwbWzR5vzZad1LjQzMzMGufx\nx9NXF5e2V5GulncAB0TE78qWXSJpCXAB8OFGBGZmZtZIzWrxWLwYVqxo3GioE12RFo9pwPwqyx/J\nXjMzM+s4g4MwZcrw5G6NUNqXWz1qVyTxuAY4QdKzI5hKWhs4PnstN0mvlHSxpAckDUl6Sw3bvFrS\nXElLJd0u6dAixzYzs8mhNGrplAbOUtbTk766zqN2RRqGPg5cCtwv6e/Zsp2ApcC+BeOYDlwPnAH8\ndLyVJW0J/BI4FTgQeB3wfUkPRsRvC8ZgZmYTWKMHD4PhFg8nHrUrMo7HjZK2BQ4CdsgW9wM/iIgl\nRYKIiEtJyQySVMMmHwbuiohPZs9vk7QnMBtw4mFmZqto9OBh4K6WInIlHpLWAE4HvhAR32tOSDV5\nOXB5xbLLgDltiMXMzLpAoyeIA7d4FFFkdtp3NCmWPDZh1QLX+UCvpKltiMfMzDpcM1s8nHjUrkiJ\nzc+BtzU6EDMzs2ZqRuKxzjrpqxOP2hUpLr0D+KykVwBzWXV22pMbEdg4HgY2rli2MfBURCwba8PZ\ns2czY8aMEcv6+vro6+trbIRmZtZRmlFcOmVKSj4mUuLR399Pf3//iGULFixo2P6LJB7vJ81QOyt7\nlAugFYnHNcB+FcteTw23886ZM4eZM2c2JSgzM+tMEc1JPGDijV5a7cP4vHnzmDWr8l9+MUXuatmq\nIUcuI2k6sA1QuqNla0k7AY9HxH2STgQ2i4jSWB2nAUdJ+gpwJrA3cADwxkbHZmZm3W/BAli5svHF\npeD5WvLKVeMhaQ1Jd0rascFx7ApcR+q6CdKcL/OAE7LXNwE2L60cEfcAbyKN33E96Tba90dE5Z0u\nZmZmTRkuvcSJRz65WjwiYnn5iKWNEhG/Z4wkKCIOr7LsD6za1WNmZraKZkwQV9LT48QjjyJ3tZwC\nHCPJ0+GYmVlXcItH5yiSPOxGqql4vaQbWfWulrc3IjAzM7NGKSUezarxeOSRxu93oiqSeDwJ/KTR\ngZiZmTXL4CCstRZMa8Ic6m7xyKfIXS2r1FuYmZl1smYMHlbixCOfmms8JG00zuurS9q9/pDMzMwa\nq1ljeICLS/PKU1z6UHnyIelGSZuXvb4+NQzgZWZm1mqtaPGIaM7+J5o8iUfldPVbAmuMs46ZmVnb\nNWNm2pLe3jQ42dKlzdn/RFPkdtqxON8zM7OO0+wWD3B3S60anXiYmZl1nGbWeDjxyCfPXS0B9Eha\nSupSCWAdSdkpp3fULc3MzNqomS0ePT3pqxOP2uRJPATcXvH8uorn7moxM7OOsnx5SgqaWeMBTjxq\nlSfxeE3TojAzM2uSZs7TAsOJx8KFzdn/RFNz4pFN5GZmZtZVmjlPC7jFIy8Xl5qZ2YTW7BaPqVNh\njTWceNTKiYeZmU1ozW7xkDx6aR5OPMzMbEIrJR7rrde8Y3i+lto58TAzswltcBBmzIDVi8zHXqPe\nXheX1qpw4iFpG0n7Slo7e+7h0s3MrOM0c/CwErd41C534iFpfUmXk8b0uATYNHvpDEknNTI4MzOz\nejVz8LASJx61K9LiMQdYAWwBLC5b/iPgDY0IyszMrFGaOUFciYtLa1ck8Xg9cExE3F+x/A7g+fWH\nZGZm1jhu8egsRRKP6Yxs6Sh5DrCsvnDMzMwaq1WJh4tLa1Mk8fgjcEjZ85A0BfgkcFVDojIzM2sQ\nF5d2liI3F30SuELSrsCawFeBF5FaPF7RwNjMzMzqEuGulk6Tu8UjIm4CtgP+BFxE6nr5KbBLRNzZ\n2PDMzMyKW7wYli1rTXHpokWwcmVzjzMR5GrxkLQ68GngzIj4YnNCMjMza4xmD5deUj5D7brrNvdY\n3S5Xi0dErCB1tTRx/DczM7PGaEfiYWMrUlx6BbBXowMxMzNrtGbPTFtSSjxc5zG+IonHr4EvS/q6\npD5Jbyl/FA1E0lGS7pa0RNJfJO02zvoHSbpe0iJJD0o6Q1KTe/HMzKybtLrFw4nH+Ip0mZyaff1E\nldcCWC3vDiW9GzgJ+BDwV2A2cJmk7SLisSrrvwI4Bzga+CXwXOB04LvAAXmPb2ZmE9PgYJocrqen\nuccp7d+Jx/iK3NUyZYxH7qQjMxs4PSLOjYhbgSNIg5S9b5T1Xw7cHRGnRMS9EXE1KfHYveDxzcxs\nAioNl97saUzd4lG7wrPTNoqkNYBZpNoRACIigMuBPUbZ7Bpgc0n7ZfvYGHgn8KvmRmtmZt2kFYOH\nwXCLh4tLx1fo7hRJ00kFpluQBhF7VkScnHN3G5C6Z+ZXLJ8PbF9tg4i4WtLBwI8krUX6OS4GPpLz\n2GZmNoG1YvAwgNVWg+nT3eJRi9yJh6RdgEuAaaTBwx4nJQ+LgUeAvIlHbpJeCHwL+BzwG2BT4Ouk\n7pYPNPv4ZmbWHVoxM22JRy+tTZEWjznAL0h1GAtI9RbLgfNIyUBejwErgY0rlm8MPDzKNscCf46I\nb2TPb5J0JPBHScdFRGXrybNmz57NjBkzRizr6+ujr6+vQOhmZtbJBgdhxx1bc6yenomRePT399Pf\n3z9i2YIFCxq2/yKJx87Av0fEkKSVwNSIuEvSJ0l3mvw0z84iYrmkucDepO4SJCl7PlrryTTgmYpl\nQ6S7asYsIZozZw4zZ87ME6KZmXWpVnW1wMRp8aj2YXzevHnMmjWrIfsvUly6nPRPHlLXyhbZ9wuA\nzQvG8Q3gg5IOkbQDcBopuTgbQNKJks4pW/8XwDskHSFpq+z22m8B10bEaK0kZmY2ybSquBRS4uHi\n0vEVafG4DtgNuAP4PfB5SRsA7wVuKhJERFyQ7ePzpC6W64F9I+LRbJVNKEtqIuIcSesAR5FqO54k\n3RVzbJHjm5nZxDM0BE884RaPTlMk8fg0UBqK5TjgXOA7pERktHE3xhURpzI8OFnla4dXWXYKcErR\n45mZ2cT25JMp+Whlceldd7XmWN0sd+IREQNl3z8CvKGhEZmZmTVAq4ZLL5koxaXN1vYBxMzMzJqh\nVRPElbirpTZFxvG4m3T3SFURsXVdEZmZmTVAq1s8XFxamyI1Ht+seL4GsAupy+VrdUdkZmbWAKXE\no9UDiEU0f26YblakxqPqIGGSjgJ2rTsiMzOzBhgchGnTYK21WnO83l5YvhyWLWvdMbtRI2s8fg28\no4H7MzMzK6yVg4fB8ERxrvMYWyMTjwNI87aYmZm1XSsHD4PU4gFOPMZTpLj0OkYWl4o0wNeGwJEN\nisvMzKwurW7xKCUeLjAdW5Hi0p9XPB8CHgV+FxG31h+SmZlZ/Vo5My24xaNWRYpLT2hGIGZmZo00\nOAjbbNO64znxqE2RrpbeWteNCJ9+MzNri1bXeLi4tDZFulqeZIwBxDLK1lmtwP7NzMzq1uoaj7XX\nhtVWc+IxniKJx+HAl0lT1l+TLdsDOBT4FHBPIwIzMzMr6pln4OmnW1vjIXn00loUSTwOAT4REf1l\nyy6WdCPwoYh4dUMiMzMzK6jVw6WXeL6W8RUZx2MPYKDK8gFg9/rCMTMzq58Tj85VJPG4D/hgleUf\nyF4zMzNrq1bPTFvS0+PEYzxFulpmAz+RtB9wbbZsd2BbPGS6mZl1ALd4dK7cLR4RcQkpybgYeE72\n+AWwXfaamZlZWw0OpmLPdddt7XFdXDq+Ii0eRMT9wHENjsXMzKwhBgdT0rFaiwd16O2Fe+5p7TG7\nTc0tHpI2kPT8imUvknSWpAskHdj48MzMzPJr9eBhJe5qGV+erpZvAx8rPZG0EfBHYDdgKnC2pPc2\nNjwzM7P8Wj14WImLS8eXJ/F4Oamuo+QQ4HFg54h4K/Bp4KgGxmZmZlZIqyeIK3GLx/jyJB6bMHJU\n0tcCP42IFdnzi0lFp2ZmZm3VrhaP3t40YurQUOuP3S3yJB5PAeX1wbszfDstpLlZpjYiKDMzs3q0\nM/GAlHxYdXkSj78AH5M0RdIBQA9wZdnr2+EBxMzMrAO0s7gU3N0yljy3034GuAI4ONvuSxHxRNnr\n7wF+38DYzMzMcotob3EpOPEYS82JR0TcIGlH4BXAwxFxbcUqPwRubmRwZmZmeT39NCxf3r7iUvAg\nYmPJNYBYRDwGXFR6Lul5wIMRMRQRv2p0cGZmZnm1a7h0cFdLLYpMElfuZmDLBsSBpKMk3S1piaS/\nSNptnPXXlPRFSfdIWirpLkmHNSIWMzPrXk48OluhIdPLqBFBSHo3cBLwIeCvpInoLpO0XdbKUs2F\nwIbA4cCdwKbUn0iZmVmXa9fMtOAaj1rUm3g0ymzg9Ig4F0DSEcCbgPcBX61cWdIbgFcCW0fEk9ni\nf7UoVjMz62ClFo921HisvjqsvbYTj7HU20LwJdLopYVJWgOYRbpjBoCICOByYI9RNnszMAAcI+l+\nSbdJ+pqkteqJxczMut/gIKyxBqyzTnuO7xlqx1ZXi0dEnNiAGDYAVgPmVyyfD2w/yjZbk1o8lgJv\ny/bxHeA5wPsbEJOZmXWp0q20akgxQH4eNn1suRMPSasBhwF7AxtR0WoSEa9tSGRjmwIMAQdGxNNZ\nXJ8ALpR0ZEQsG23D2bNnM2PGjBHL+vr66Ovra2a8ZmbWIu0aPKyk2xOP/v5++vv7RyxbsGBBw/Zf\npMXjW6TE41fATaSh0uvxGLAS2Lhi+cbAw6Ns8xDwQCnpyNxCKnZ9HqnYtKo5c+Ywc+bM4tGamVlH\na9fgYSXdPkNttQ/j8+bNY9asWQ3Zf5HE4z3AuyLikkYEEBHLJc0ltaBcDCBJ2fOTR9nsz8ABkqZF\nxOJs2fakVpD7GxGXmZl1p3bNTFvS7S0ezVakuPQZ4J8NjuMbwAclHSJpB+A0YBpwNoCkEyWdU7b+\n+cAgcJakHSW9inT3yxljdbOYmdnE1+4WDxeXjq1I4nEScHTWKtEQEXEB8J/A54HrgJcC+0bEo9kq\nmwCbl62/CNiHNFvu34D/I42oenSjYjIzs+7UCYmHWzxGV6SrZU/gNcB+kv4BLC9/MSLeXiSQiDgV\nOHWU1w6vsux2YN8ixzIzs4nLxaWdrUji8STws0YHYmZmVq+VK+HJJ9tb49HtxaXNljvxqNb6YGZm\n1gmeeAIi3OLRyTy3iZmZTRjtnCCupLcXnnkGlvlWh6oKjVwq6QDgXcAWwJrlr0WEB8kwM7O2aOcE\ncSWlGWoXLoSpU9sXR6fK3eIh6WPAWaQhzXchzSY7SBrG/NcNjc7MzCyHTmnxAHe3jKZIV8uRwIci\n4qOkMT2+GhH7kAb7mjHmlmZmZk3UzplpS3p60lcnHtUVSTy2AK7Ovl8CZKeY/wM84YmZmbXN4GCa\nlXbNNcdft1nc4jG2IonHw6RZYAH+Bbw8+34r0lwpZmZmbdHuwcNgZI2HrapI4nEl8Jbs+7OAOZJ+\nC/wIj+9hZmZt1O7Bw8AtHuMpclfLh8gSlog4RdIg8G+kCd5Ob2BsZmZmubR7gjiAadNgyhQnHqMp\nMoDYEGkW2NLzHwI/bGRQZmZmRQwOwoYbtjcGyaOXjqXQAGKSXinpPEnXSHputuy9kvZsbHhmZma1\n64QaD/DopWMpMo7HO4DLSHe07AKUhkeZAXy6caGZmZnl0wk1HpASDxeXVlekxeO/gSMi4oOMnJn2\nz4BHLTUzs7Zxi0fnK5J4bA/8ocryBcC69YVjZmZWzNKlsHhx+4tLwYnHWIqO47FNleV7AnfVF46Z\nmVkxnTBceomLS0dXJPH4HvAtSS8DAthM0kHA14HvNDI4MzOzWnVS4uEWj9EVGcfjy6SE5QpgGqnb\nZRnw9Yj4dgNjMzMzq1knzExb4uLS0RUZxyOAL0r6GqnLZR3g5oh4utHBmZmZ1aoTJogrcYvH6Iq0\neAAQEc8ANzcwFjMzs8IGB9OIoet2wG0OTjxGV3PiIenMWtaLiPcVD8fMzKyYwUFYb72UfLRbT0/q\nahka6ox4OkmeFo/DgHuB6/AstGZm1mE6ZfAwSC0eEbBoUUpCbFiexOM7QB+wFWlW2vMi4vGmRGVm\nZpZTpwweBsMz1C5c6MSjUs0NQBFxFLAp8FXgzcB9ki6QtK8kt4CYmVlbdcLMtCWlxMN1HqvK1fMU\nEcsioj8i9gFeCPwDOBW4R9I6zQjQzMysFp3Y4uHEY1X1lLwMkQYQE7BaY8IxMzMrppMSj1L3ihOP\nVeVKPCRNldQn6bfA7cBLgI8AW3gcDzMza6dOKy4FJx7V1Jx4SDoVeAg4FvglsHlEvDMiLomIoXoD\nkXSUpLslLZH0F0m71bjdKyQtlzSv3hjMzKw7RaTEo1NqPEotHh69dFV57mo5AvgXaSK4vYC9qtWU\nRsTb8wYh6d3AScCHgL8Cs4HLJG0XEY+Nsd0M4BzgcmDjvMc1M7OJ4amnYMWKzmnxWHNNWGstt3hU\nkyfxOJdU09EMs4HTI+JcAElHAG8C3ke6i2Y0pwE/INWbvLVJsZmZWYfrpAniSjx6aXU1Jx4RcVgz\nApC0BjAL+FLZsULS5cAeY2x3OGlMkYOAzzQjNjMz6w6dmHj09DjxqKaugVyzQtPpdcawAemumPkV\ny+cDm4xy3G1JicpBjagvMTOz7tZJM9OWuMWjunpHkD+dFtdWSJpC6l45PiLuLC1uZQxmZtZZOmlm\n2pLeXheXVlN4dtpMI/7hPwasZNUEZmPg4Srr9wC7AjtLOiVbNgWQpGeA10fE70Y72OzZs5kxY8aI\nZX19ffT19RWL3szM2m5wEKZOhWnT2h3JsG5t8ejv76e/v3/EsgULFjRs//UmHnWLiOWS5gJ7AxdD\nyiCy5ydX2eQp4MUVy44CXgO8A7hnrOPNmTOHmTNn1hm1mZl1ktLgYZ00gUdvL9x/f7ujyK/ah/F5\n8+Yxa9ashuy/3sRjP+CBBsTxDeDsLAEp3U47DTgbQNKJwGYRcWhEBHBz+caSHgGWRsQtDYjFzMy6\nTCcNHlbi4tLqctd4SLpS0roAEfGniFiWLe+VdGWRICLiAuA/gc8D1wEvBfaNiEezVTYBNi+ybzMz\nm/g6abj0km7tamm2Ii0erwbWrLJ8LeCVRQOJiFNJE85Ve+3wcbY9ATih6LHNzKy7ddLMtCUuLq2u\n5sRD0kvLnr5QUvmtrqsBb6Ax3S5mZma5DA7CFlu0O4qR3OJRXZ4Wj+tJI5cGUK1LZQnw0UYEZWZm\nlkendrUsXQrPPJOGULckT+KxFen22buA3YFHy157BngkIlY2MDYzM7OadGpxKaTulk6LrZ3yDJl+\nb/ZtvYOOmZmZNcyKFbBgQWfWeEDqbnHiMazQ7bSSXgB8HNgxW3Qz8K2ykUTNzMxaohOHS4fhxMMF\npiMVuZ12X1KisTtwQ/Z4GfAPSfs0NjwzM7OxdeIEcTCyxcOGFWnx+DIwJyKOLV8o6cvAV4DfNiIw\nMzOzWnR6i4cTj5GK1GvsCJxRZfmZwAvrC8fMzCyfTm3xKBWXOvEYqUji8Siwc5XlOwOP1BeOmZlZ\nPqXEY7312htHpenT09wxTjxGKtLV8j3gu5K2Bq7Olr0COIY054qZmVnLDA6mbo011mh3JCNNmZJa\nPVxcOlKRxOMLwELgP4ATs2UPAp+j+myyZmZmTdOJg4eVePTSVeVOPLLZYecAcyT1ZMucz5mZWVt0\n4uBhJU48VlVoHA8ASRsC22ff3xoRjzUsKjMzsxp14gRxJT09TjwqFRnHY7qkM4GHgD9kj4cknSFp\nWqMDNDMzG4u7WrpLkbtavgHsBbwZWDd7vDVbdlLjQjMzMxtfpyceLi4dqUhXyzuAAyLid2XLLpG0\nBLgA+HAjAjMzM6tFp9d4PPhgu6PoLEVaPKYB86ssfyR7zczMrGU6vcXDXS0jFUk8rgFOkLRWaYGk\ntYHjs9fMzMxaYvFiWLrUxaXdpEhXy9HAZcD9kv6eLdsJWArs26jAzMzMxtOpw6WXuMVjVUXG8bhJ\n0rbAQcAO2eJ+4AcRsaSRwZmZmY2lGxKPhQshIg2fbgXH8YiIxaSh00eQtLaTDzMza5VOnZm2pLcX\nhoZSl9D06e2OpjMUqfFYhaSpkv4DuLsR+zMzM6tFqcWjU2s8envTV3e3DKs58ciSixMlDUi6WtLb\nsuWHkxKOj5OGUjczM2uJwUFYbTWYMaPdkVTX05O+OvEYlqer5fPAvwO/Jc1Ge6Gks4CXA58ALoyI\nlY0P0czMrLrScOmdWj/hFo9V5Uk83gkcEhEXS3oxcEO2/U7ZxHFmZmYt1cmDh8Fw4uHRS4flqfF4\nHjAX0p0twDJgjpMOMzNrl04ePAzc4lFNnsRjNeCZsucrgKcbG46ZmVntOnlmWnCNRzV5uloEnC1p\nWfZ8LeA0SYvKV4qItzcqODMzs7EMDsL227c7itFNnQprrunEo1yeFo9zSPOxLMge5wEPlj0vPQqR\ndJSkuyUtkfQXSbuNse7+kn4j6RFJC7K7bF5f9NhmZtadOr2rBTx6aaWaWzwi4vBmBSHp3cBJwIeA\nvwKzgcskbRcRj1XZ5FXAb4BPAU8C7wN+IWn3iPh7lfXNzGwC6vTiUhgevdSShgwg1gCzgdMj4tyI\nuBU4AlhMSihWERGzI+LrETE3Iu6MiOOAO4A3ty5kMzNrp6GhlHh0co0HuMWjUtsTD0lrALOAK0rL\nsjtlLgf2qHEfAnqAx5sRo5mZdZ4FC1Ly0Q0tHk48hrU98QA2IN0xM79i+Xxgkxr38V/AdOCCBsZl\nZmYdrNMniCvp6XHiUa4TEo+6SDoQ+AzwzlHqQczMbALqlsTDLR4jFZqdtsEeA1YCG1cs3xh4eKwN\nJb0H+C5wQERcVcvBZs+ezYyKQf37+vro6+urOWAzM2u/Tp+ZtqS3F26/vd1R1K6/v5/+/v4RyxYs\nKHzT6irannhExHJJc4G9gYvh2ZqNvYGTR9tOUh/wfeDdEXFprcebM2cOM2fOrC9oMzNru06fmbak\n21o8qn0YnzdvHrNmzWrI/tueeGS+QRqcbC7Dt9NOA84GkHQisFlEHJo9PzB77WPA3ySVWkuWREQX\nvb1mZlbU4CCsvXZ6dLJuSzyarSMSj4i4QNIGpBlwNwauB/aNiEezVTYBNi/b5IOkgtRTskfJOYxy\nC66ZmU0s3TB4GLi4tFJHJB4AEXEqcOoorx1e8fw1LQnKzMw6VjcMHgapxWPJElixAlbvmP+67dP1\nd7WYmdnk1OkTxJWUZqj16KWJEw8zM+tK3dLVUko83N2SOPEwM7Ou5MSjOznxMDOzrtQtiUdPT/rq\nxCNx4mFmZl2pm4pLwTUeJU48zMys6zzzTPpH3k3FpW7xSJx4mJlZ1+mW4dIB1lknfXXikTjxMDOz\nrtMtE8QBTJniQcTKOfEwM7Ou000tHuDEo5wTDzMz6zrdMkFcSW+vi0tLnHiYmVnXKSUe663X3jhq\n5YnihjnxMDOzrjM4COuu2z1znzjxGObEw8zMuk63DB5W4sRjmBMPMzPrOt0yeFiJi0uHOfEwM7Ou\n0y0z05a4uHSYEw8zM+s67mrpXk48zMys6zjx6F5OPMzMrOt0W41HKfGIaHck7efEw8zMukpE99V4\n9PTAypWwZEm7I2k/Jx5mZtZVFi1Ks9N2W4sHuMAUnHiYmVmX6aYJ4kpKiYfrPJx4mJlZl3Hi0d2c\neJiZWVfptplpwYlHOSceZmbWVbptZlpIxaXgxAOceJiZWZcZHEyTw5X+mXcDF5cOc+JhZmZdpTR4\nmNTuSGo3dSqssYZbPMCJh5mZdZluGzwMUpLk0UsTJx5mZtZVum3wsBInHknHJB6SjpJ0t6Qlkv4i\nabdx1n+1pLmSlkq6XdKhrYp1Munv7293CF3J5y0/n7NiJuN5q3eelnads54eJx7QIYmHpHcDJwHH\nA7sAfwcuk7TBKOtvCfwSuALYCfgW8H1J+7Qi3slkMv5RawSft/x8zoqZjOetWxOP3l4Xl0KHJB7A\nbOD0iDg3Im4FjgAWA+8bZf0PA3dFxCcj4raIOAX4cbYfMzObwLptZtoSd7UkbU88JK0BzCK1XgAQ\nEQFcDuwxymYvz14vd9kY65uZ2QTRjcWl4MSjZPV2BwBsAKwGzK9YPh/YfpRtNhll/V5JUyNi2WgH\nu+WWomFOTgsWwLx57Y6i+/i85edzVsxkO28R8MQT3VtcesklsPfe6ecoPaC25/V6wQvgvPPq30+9\nOiHxaJW1AA4+2JlHPguYNWsS/VVrGJ+3/HzOipmc523ZsuIJ14IFC5jXhmxtt93g7rvT99LwOCSV\n35cvKx+rpN5xS6ZNK37Obhn+1L5WfVF0RuLxGLAS2Lhi+cbAw6Ns8/Ao6z81RmvHlunLwUVinORm\ntTuALuXzlp/PWTGT77x99KP1bT9r1uQ7ZwDf+17du9gSuLqeHbQ98YiI5ZLmAnsDFwNIUvb85FE2\nuwbYr2LZ67Plo7kMOAi4B1haR8hmZmaTzVqkpOOyenekaETHUb1BSO8CzibdzfJX0t0pBwA7RMSj\nkk4ENouIQ7P1twRuBE4FziQlKd8E3hgRlUWnZmZm1iHa3uIBEBEXZGN2fJ7UZXI9sG9EPJqtsgmw\nedn690h6EzAH+BhwP/B+Jx1mZmadrSNaPMzMzGxyaPs4HmZmZjZ5OPEwMzOzlpkUiUfeCegmO0nH\nSxqqeNzc7rg6iaRXSrpY0gPZ+XlLlXU+L+lBSYsl/VbSNu2ItZOMd94knVXl2rukXfF2AkmfkvRX\nSU9Jmi/pZ5K2q7Ker7dMLefM19qqJB0h6e+SFmSPqyW9oWKduq+zCZ945J2Azp51E6nQd5PssWd7\nw+k400lF0EcCqxRKSToG+AjwIWB3YBHpuluzlUF2oDHPW+bXjLz2+loTWsd6JfBt4GXA64A1gN9I\nWru0gq+3VYx7zjK+1ka6DzgGmEkaHOZK4CJJO0LjrrMJX1wq6S/AtRFxdPZcpJN7ckR8ta3BdShJ\nxwNvjYiZ7Y6lG0gaAt4WEReXLXsQ+FpEzMme95KG9T80Ii5oT6SdZZTzdhYwIyLe3r7IOlv2oekR\n4GiQ/tgAAA39SURBVFUR8adsma+3MYxyznyt1UDSIPCfEXFWo66zCd3iUXACOku2zZrD75R0nqTN\nx9/EACRtRfr0VH7dPQVci6+7Wrw6ax6/VdKpkrpwVo6mWpfUWvQ4+Hqr0YhzVsbX2igkTZH0HmAa\ncHUjr7MJnXgw9gR0m7Q+nK7xF+AwYF/SoG5bAX+QNL2dQXWRTUh/5Hzd5fdr4BDgtcAngb2AS7KW\nykkvOw/fBP4UEaW6K19vYxjlnIGvtaokvVjSQmAZaZDO/SPiNhp4nXXEAGLWWSKifEjcmyT9FbgX\neBdwVnuissmgorn2H5JuBO4EXg1c1ZagOsupwAuBV7Q7kC5S9Zz5WhvVrcBOwAzSCOLnSnpVIw8w\n0Vs8ikxAZxUiYgFwOzBpq+RzehgQvu7qFhF3k36PJ/21J+l/gTcCr46Ih8pe8vU2ijHO2Sp8rSUR\nsSIi7oqI6yLiONINGUfTwOtsQiceEbEcKE1AB4yYgK6u2fUmE0nrkH4Zx/zFtST7A/YwI6+7XlKF\nva+7HCQ9D1ifSX7tZf9A3wq8JiL+Vf6ar7fqxjpno6zva626KcDURl5nk6Gr5RvA2Uoz4JYmoJtG\nmpTOqpD0NeAXpO6V5wInAMuB/nbG1UmyepdtSJ8AALaWtBPweETcR+pT/m9J/yTNiPwF0pxCF7Uh\n3I4x1nnLHscDPyH9gdsG+Aqpta3uGTG7laRTSbd5vgVYJKn0iXNBRJRm2vb1Vma8c5Zdh77WKkj6\nEqn25V9AD2lG971Is79Do66ziJjwD9KYAfcAS4BrgF3bHVMnP0gJxv3Z+foXcD6wVbvj6qRH9ss4\nROrKK3+cWbbO54AHgcWkP2bbtDvudj/GOm+kabcvJf0jWArcBXwH2LDdcbf5nFU7XyuBQyrW8/VW\n4znztTbqefv+/2/vzKOtrqo4/vmGsIIcWFQOtcCJBEvCgrA0BVmaimgplrNYLc1cTiiZM9bSlFIJ\nTc0pFBARUhMHXIrLTHEONEXE6TGFggoyz+z+2OfK4Xbfe/cN3veA/Vnrrvf7/c75nbPP/p37zj57\nn989SRfLk24eB3oX5WlwP9vkf8cjCIIgCILmwya9xiMIgiAIguZFGB5BEARBEFSMMDyCIAiCIKgY\nYXgEQRAEQVAxwvAIgiAIgqBihOERBEEQBEHFCMMjCIIgCIKKEYZHEARBEAQVIwyPYLND0iBJkxqx\nvJ6S1qZ9CxoNSVWSzmrMMoP609j9ZlNE0jpJhze1HEHzJgyPoFkg6SuSVkpqLWkLSUvSpk013TMo\n/aNbK2l1GqivS/sw1MSfyDY6agQmAjuY2aJGLLMsJG0l6UpJUyUtlzRH0uOSjqi0LM2Z1FcmN7CY\nBvcbSf2zPrs2Pa/Rkto3ULbmwvb4Xh9BUC2bwyZxwcbBD4BXzWy5pB7AJ2Y2u4z73sAHg5bAPsAw\nfB+G00tlltTCzJbh+ww0Cma2BpjXWOWVi6RtcKNnK+Bi4BVgDdALGCzpyaYwhpoxDdofohH7zUJg\nN3zitzO+R8gY/DvwuSGppfmO3Z8bZlbx70Gw8REej6C5sDc+iALsmx3Xxhoz+8jM5pjZWGAkvhU2\nknql2eXBkl6RtALYp3j2K2mYpAcknZdmoB9L+oukFlmeVpIGS5opaYWktyX9PKX1TPVsnc77S1og\n6ccp33JJj+UeHEm7SPqHpA8lLZb0kqS6zqavAjoAPcxspJm9ZWbvmtntwJ7AklRXW0nDJc2XtFTS\no5I6ZrIU5D1U0lspz5jkfeqfPEnzJQ2VpOy+KkmXSBqVPFSzJW1g8ElqL+nB1MaFku6VtG2WPkjS\nZEknpPI+lXRP7rWSc6Gk9yUtS/n7ZekF/feW9HKSf6KkbxTah+9E2jXzNpyU0i6XNCM909mS/lyd\nsuvTb6rBUp+da2Yv4Btz9ZC0ZVZ2K0nXJJmWSHpeUs8ieU5J/XFJel7nSFpQQre/lFTY+KscfbaV\ndLekeSl9WtIhklqmNs5J/bpK0m+zezcItUjaQ9KTqZyPJd1S9Gzrq8NgIyY8HkGTIXcv/yedtgHW\nyAfz1sA6SfOBUWZ2Rh2KXQm0SseFGe5VwEB818UFwP78/+x3f3zHxV74FtljgMnAHSl9BLAXcEaS\nuQOwXXZ/cXltgIuAE4DV+Kz2HtyoAtgSeAS4EFgFnASMk9SpHE9PMgCOBkaa2dzi9DQ7L3AXsCvQ\nF1gM/BF4VNLuZrY2k/dM4GfA1sAD6bMAOATYBbgfeBYYm5U9ELgSuAw4GBgqaZqZPZlkHAcsSu1u\nCdwEjAZ6Z2XsihuLfYB2qfwLgEtT+kXAccCpwLvAfsAISfPM7JmsnCuAAcDHwC34jrf7AvcCewAH\n4d4xAQslHQWck9r8Jh4m6FqsyyLq2m9qJBlh/Vi/e2qBG4HOSbYPgCOA8ZK6mNl7kvbB+9RvgIeA\nA/Atyovl6wgcme4vlF+bPq9IdR8EfJLKaJ3uPRvvR0cBs4D26VOqbW3w3UsnAt3w78sdwA3AL7Ks\nDdJhsBHS1Nvwxmfz/eAetw5AF3xr6m/hA9xCPGzSAWhXw/2DgEnZeTc85DE6nRe2YO9by33DcKNE\n2bV7caMH3C2+Dti/Gjl64v/Ut07n/dN59yxPp1RG9xra8zpwenZeBZxVTd6vpvLOrkXHHVO+vbJr\n7YClQL8ieXfK8tyMGymts2vjgZuK5HukqL57gIfT8YG4UfW1LH33JE+37FksBtpkeQYDz6XjVrjn\nZq+iem7Dja5c/72y9EPStValnnm6NgCYCrQos7/Wqd9UU0b/1P5FqV2F7duvy/K0x43V7YvufQK4\nItPzuKL0EcD8InlXkH2HytTng8Dt1cg/FHiihvatAw5Px6fgRuAXi57LGtL28/XRYXw2/k+EWoIm\nw8zWmdlMfDB62cymADsAc81sopnNNLP5tRTzbUmLJC0DXsBnV2fm1QD/LkOcKZb+6yU+AAohga74\nP8t/lVFOgTVm9spnQphNAz7F24qkLyVX+pvyMMdifJbZoczyVXsWSPWtBl7KZJkPTCvIklhmZtOz\n87nAdDNbXnRtWzbk+RLnhXI7A7PMbE5W91QyPSSm24Yemlz3HXFvzBMpXLM46epE3EjNeb2oDErI\nmzM2lV0l6VZJP6mHi7+mflMdi/A+1Q04F5gEXJKldwFaAG8XtXk/1re5E9kzTRSfA8wo+g6Vo8+b\ngWNTCGawpHztyZ3Ad1L4ZaikA2toZ2fgNTNbkV2biE84OmXX6qPDYCMmQi1BkyHpDWBH3AWv9A9w\nC6BFOp5uZl1qKeYt4DB81jjHfKFnMUvLEKd40Z2xfg3Uchqfa3G3/3nAe6mO+1gfJqqNj/ABvHMj\nyVOq/TXppDGpqZ7Cuoc+uDs+Z2UN5RQGsmrlNbPZknbDwxQH4uGNgZJ62voQVENkr451ZlaVjqfJ\n19v8FQ+3gbd5DfBd3IOQs6RMuQoU9/1a9Wlmj0nqkPIcCEyQdKOZnW9mkyXthHsuDgDGSJpgZj+t\no1w5lepnQTMhHm7QlByCz/w+BI5Px2/gceSu+D++2lhlZlXJO1LK6GgMXse/Kz1ry5ixhaTuhRNJ\nnYC2+FoC8MW0d5rZuOTpmQfsVG7haYY4Gjhe0vbF6cmj8gU8lLAFvj6lkPZlfMY5pQ7tqY7vlzif\nmo6nAu0lfT2r+5u4Hsqt+018QNzRzN4v+vy3DnKuwr0IG2BmK83sETM7B19rsDfucagkVwNHS9oz\nnU/GZd2uRJsLb41MA75XVE6PMuoqS59m9omZjTCzk/CQ1KlZ2hIzG2tmv8LXGfWT1LZEXVPxBb2t\ns2s/xCcJ08qQNdhECY9H0GSY2aw0aG6HL0IUvs7jfiuxYLKelBuSqBYzmyFpOPA3SWcDr+Gemm3N\n36QpVc8a4IaUfy2+oO45MyuEfd4BjpT0cDr/fT1kvRg3hl6UdAn+Ou1q3CV/Ab6e5F1J44DbJJ2G\nz5ivxhcGjqtjfaXYR9JAfF3Aj/BFh30AzGxC8mrdLWkA7tm6EXjKzMr6TQ0zWyLpGmBICoM8C2yD\nrwFaaGYjUtZSusuvTQd2ltQVmI2vKzkWH+BfxF+TPTH9nVFm2xuF5Hl5AF8cepiZvSNpFDA86XYy\nHnrojYcuxuP96emk14dw79nB1PLKcDn6lPQ7PDw5BX81vS/JYE71fZBkMtLiVzP7tER1dwOXA3el\nMrcFrgeGm9lH9VBVsIkQHo+gqekJvGRmq/AZ3KxGNDqggb/dkHEa8Hd84JwK3IrHyqurZym+SHIU\n8Awe1z8mSz8Xf2NkIj5oP4bH+suW3cwW4B6GkbgRMglfh3IccJmt/w2Pk/GB5KFU3zrg0DqEE2ri\nWqA7PhBdBAwwswlZ+uF4O58GHsffojimuJCaMLNL8UH5AnwAHI8bN1V5tlK3Zsf34Tp+CvcuHYOH\nqk7BB9/X8IG9b9JrpRkC9Mm8ZCcDw4Fr8HDi/bieZwKY2XN4nxwAvIobfUPwxaQ1UoY+VwF/wHXy\nT9yIPjalLQbOB17GDbZCSOaz4rN6luNvxrTD15+MwRfI5muwgs0QbbimJwiChpJ+82CImbVralk+\nTyRV4e28vqllCUDSbcBuZlaXkGAQVJwItQRBEGyESDoP9yAsxb0OJwK/blKhgqAMwvAIgqC+hLu0\naemB/4DYVvhvYZxpZsOaVqQgqJ0ItQRBEARBUDFicWkQBEEQBBUjDI8gCIIgCCpGGB5BEARBEFSM\nMDyCIAiCIKgYYXgEQRAEQVAxwvAIgiAIgqBihOERBEEQBEHFCMMjCIIgCIKKEYZHEARBEAQV43+E\nffm/E7WhNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f943002fb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot pcr\n",
    "plt.plot(pcr_reg_results[0])\n",
    "plt.title(\"PCR: RMSE by # of Principal Components\")\n",
    "plt.xlabel(\"# Principal Components in Regression\")\n",
    "plt.ylabel(\"Root-Mean-Square Error (RMSE)\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion/Conclusion\n",
    "\n",
    "PLS is a powerful algorithm for linear modeling, particularly in high dimensional settings where the number of predictors is larger than the number of samples. PLS harnesses the power to decompose the matrix of predictors into a lower dimensional settings while making sure that the decomposition is driven by an inner relation between the decomposed matrix and the response variable to assure higher predictor potential. This inner relation is derived from the relationship between the predictor and response matrix scores with a component varying regression coefficient $U_h = b_h t_h$. This interaction between the predictors and response distinguishes PLS from other methods that decompose the predictive space and response space separately, such as PCA. As an iterative procedure, each decomposed and rebuilt component receives the scores and weights of the previous component, often improving predictive accuracy.\n",
    "\n",
    "Important for PLS model evaluation is the proper selection of the number of components to use in the model. The maximum number of components that can be accessed in PLS is the rank of the design matrix of predictors. The maximum number of components, however, is often not needed in real world data set to the presence of noise and collinearity. In this report, we used the root means squared error (RMSE) of the model used on a test data set in order to determine the proper component level. The training data will often see a lower RMSE as the component level increases, which reflects overfitting of the data, an unfortunate outcome if the intention is to the model for prediction. Historically, PLS has been used in chemometrics to analyze the presence of different compounds contained in test samples. In our model of the Ames, Iowa housing prices, we determined that out of 30 predictors the optimal number of components was 7. This demonstrates the practical use of PLS for applications that use real world data outside of its historical uses.\n",
    "\n",
    "In addition, the PLS algorithm generates residuals for the predictor matrix ($E_h$)and response variables ($F_h$), which is derived from the difference between the additive decomposition of the respective matrices and the original data matrices. Fh can be interpreted as how much variation the model explains in the response variable while Eh gives an indication of how much of the X block was not used in the model, and therefore how much signal is present in the data.\n",
    "PLS in comparison to PCR and ordinary least squares fared well, producing an RMSE much lower than PCR and slightly above OLS. In our simulated data, we demonstrate the high-dimensional scenario when the number of observations is lower than the number of predictors and multicollinearity is present. It is demonstrated that PLS allows use to first run the model, which would be impossible using OLS and other methods. In addition, we are able to recover a number of components closer to the true number of variables controlling the data while maintaining high predictive power.\n",
    "\n",
    "Because this algorithm can deal with matrices of considerable size, and even multiple response variables, speed, and efficiency are important. To optimize our model employed a number of methods including using the efficient mathematical package Numpy for matrix operations, efficient memory allocation, and just-in-time programming using the Numba package to compile the code to LLVM IR which is natively executed at runtime, faster than normal Python code. Through these optimizations we were able to improve our runtime performance of the PLS algorithm by nearly 50 percent.\n",
    "\n",
    "# References\n",
    "\n",
    "[1] Geladi, P., and Kowalski, B. R.. 1986. “Partial Least-Squares Regression: a Tutorial,” Analytica Chimica Acta, 185, 1 (1986).\n",
    "\n",
    "[2] Wold, S. et al. 2001. PLS regression: a basic tool of chemometrics. Chemometr. Intell. Lab. 58: 109–130.\n",
    "\n",
    "[3] Tobias, R. D.. 2003. An Introduction to partial least squares regression <http://www.ats.ucla.edu/stat/sas/library/pls.pdf>.\n",
    "\n",
    "[4] Mevik, B-J, Wehrens R, Liland KH. 2011. R Package: ‘pls’: Partial Least Squares and Principal Component regression.\n",
    "\n",
    "[5] DeCock, D.. 2011. Ames, Iowa: Alternative to the Boston housing data as an end of semester regression project. J. Statist. Ed. 19(3):1–15.\n",
    "\n",
    "[6] Crouser, J. 2016. Lab 11 - PCR and PLS Regression in Python. \n",
    "<http://www.science.smith.edu/~jcrouser/SDS293/labs/lab11/Lab%2011%20-%20PCR%20and%20PLS%20Regression%20in%20Python.pdf>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
